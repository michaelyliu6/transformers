{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean GPT2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is GPT-2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to GPT-2\n",
    "\n",
    "[GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), which stands for Generative Pre-trained Transformer 2, was released by OpenAI in February 2019 and marked a pivotal moment in natural language processing (NLP) and the entire broader AI community. This transformer-based language model demonstrated unprecedented capabilities in generating coherent, human-like text, setting new standards for what AI could achieve in language understanding and generation.\n",
    "\n",
    "## How GPT-2 Works\n",
    "At its core, GPT-2 operates like a highly sophisticated text prediction system. Given a sequence of words, it predicts the most likely next word. The model achieves this through:\n",
    "\n",
    "1. A transformer architecture that processes text using [attention](https://arxiv.org/pdf/1706.03762) mechanisms, allowing it to consider relationships between words regardless of their distance in the text\n",
    "2. **Unsupervised learning** on a massive dataset of internet text, enabling it to capture patterns in human language without explicit training labels\n",
    "3. A large-scale architecture (up to 1.5 billion parameters) that can store and utilize complex language patterns.\n",
    "\n",
    "- Transformer-based: GPT-2 is built upon the [Transformer architecture](https://arxiv.org/pdf/1706.03762), a neural network design that excels at understanding context and relationships within sequences of data, like text. This architecture allows it to process and generate text much more effectively than previous models.\n",
    "- Generative: The \"G\" in GPT-2 stands for \"generative.\" This means it's designed to generate new text that is coherent and often remarkably similar to human-written text. It can continue a story, answer questions, write poems, translate languages, and perform various other text-based tasks.\n",
    "- Pre-trained: The \"P\" stands for \"pre-trained.\" GPT-2 was trained on a massive dataset of text from the internet (estimated to be around 40GB of text from 8 million web pages). This pre-training allows it to learn the nuances of language, including grammar, facts about the world, and even different writing styles.\n",
    "- Unsupervised Learning: It learns patterns and structures in the data without explicit human labeling, simply by trying to predict the next word in a sequence, repeatedly extending the text one token at a time (\"autoregressive\").\n",
    "\n",
    "\n",
    "Significance of GPT-2:\n",
    "\n",
    "- **Unprecedented Text Generation Quality:** GPT-2 was a significant leap forward in the quality of text generated by AI. Its output was often surprisingly coherent, fluent, and contextually relevant, sometimes even difficult to distinguish from text written by a human. This sparked excitement and concern about the potential implications of such technology.\n",
    "- **Phased Release and Ethical Considerations:** OpenAI initially withheld the full version of GPT-2 due to concerns about potential misuse. They were worried about its ability to generate convincing fake news, spam, and other forms of malicious content. This phased release, where progressively larger versions were released over time, was a novel approach in the AI community and highlighted the growing ethical considerations surrounding powerful AI models. It triggered a wider discussion about responsible AI development and deployment.\n",
    "- **Impact on NLP Research:** GPT-2 served as a catalyst for further research in natural language processing. It demonstrated the power of large-scale, pre-trained Transformer models and inspired the development of even larger and more sophisticated models like GPT-3, BERT, LaMDA, and others. It shifted the focus of NLP research towards scaling up models and datasets.\n",
    "- **Zero-Shot and Few-Shot Learning:** GPT-2 showed promising results in zero-shot and few-shot learning.\n",
    "    - Zero-shot learning: The model could perform tasks it wasn't explicitly trained for, simply by being given a description or a few examples in the prompt.\n",
    "    - Few-shot learning: With just a few examples, the model could quickly adapt to new tasks. This ability to generalize to new tasks with minimal training data was a significant advancement.\n",
    "- **Applications and Potential**: GPT-2, and subsequent models, have paved the way for numerous applications, including:\n",
    "    - Chatbots and conversational AI: More engaging and human-like conversations.\n",
    "    - Content creation: Assisting with writing articles, stories, scripts, and marketing copy.\n",
    "    - Code generation: Helping programmers write and debug code.\n",
    "    - Language translation: Improving the accuracy and fluency of machine translation.\n",
    "    - Text summarization: Generating concise summaries of large amounts of text.\n",
    "\n",
    "\n",
    "## Historical Significance\n",
    "\n",
    "GPT-2's release was notable for several reasons:\n",
    "\n",
    "- It demonstrated that scaling up model size and training data could lead to qualitatively predictably better performance (\"[Scaling Laws](https://arxiv.org/pdf/2001.08361#page=3&org=openai)\")\n",
    "- It challenged the prevailing wisdom in AI research by showing that simple architectures at scale could outperform complex architectural innovations\n",
    "- Its capabilities were significant enough that OpenAI initially delayed the full release due to concerns about potential misuse\n",
    "- It helped establish the foundation for modern language models and showed the potential of unsupervised learning for NLP tasks\n",
    "\n",
    "## Impact on AI Development\n",
    "GPT-2's success triggered a series of transformative changes in the field:\n",
    "- **The Scaling Race** - GPT-2 sparked an industry-wide competition to build increasingly larger models. This \"scaling race\" led to rapid advancements, with organizations like Google, OpenAI, and Anthropic pushing boundaries in model size and capability. The focus shifted from architecture innovation to scaling existing architectures effectively.\n",
    "\n",
    "- **Emergence of Meta Learning** - One of GPT-2's most surprising discoveries was its ability to perform \"[few-shot learning](https://arxiv.org/pdf/2005.14165)\" – adapting to new tasks with minimal explicit instruction. This phenomenon, later explored more deeply with GPT-3 and GPT-4, suggested that large language models could develop *meta-learning* capabilities, learning how to learn during pre-training.\n",
    "\n",
    "- **Emergent Capabilities** - GPT-2 began revealing what we now call \"emergent abilities\" – capabilities that appear suddenly above certain scale thresholds. This observation, formally documented in the [GPT-4 technical report](https://arxiv.org/pdf/2303.08774), suggested that scaling language models could lead to qualitatively new behaviors that are difficult to predict in advance. For instance, the ability to perform basic arithmetic or follow implicit reasoning steps emerged without explicit training for these tasks.\n",
    "\n",
    "\n",
    "## Broader Implications\n",
    "\n",
    "The success of GPT-2 influenced the development of multimodal models, showing how scaling could benefit other domains beyond text\n",
    "It sparked important discussions about AI safety and ethics, leading to more thoughtful release strategies for powerful AI systems\n",
    "The model demonstrated that unsupervised pre-training could capture significant world knowledge, laying groundwork for future work in knowledge representation and reasoning.\n",
    "\n",
    "These developments fundamentally changed how researchers and organizations approach AI development, shifting focus from small, specialized models to large, general-purpose systems capable of emergent behaviors and meta-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup (don't read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import sys\n",
    "import webbrowser\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Callable\n",
    "\n",
    "import datasets\n",
    "import einops\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "from jaxtyping import Float, Int\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.utils import gelu_new, tokenize_and_concatenate\n",
    "\n",
    "# Run on \"mps\" (Mac M series) or \"cuda\" (NVIDIA GPUs) if available, else run on \"cpu\"\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing: The Tokenizer \n",
    "GPT2's input is natural language (i.e. a sequence of characters, strings, etc), but ML models usually take in vectors as input. To convert natural language into vectors, the **tokenizer** splits up the lanuguage into units called **tokens**, and then converts the list of tokens into vectors. \n",
    "\n",
    "### Splitting language to tokens\n",
    "A token is a substring that is a member of the **vocabulary**  set. But what is a good implementation for how to create a **vocabulary**?\n",
    "\n",
    "Can we take a set of all every word in every dictionary ever made, and have each word be a token? No, this wouldn't allow us to be able to handle arbitary text (i.e. typos, punctuations, URLs, etc). \n",
    "\n",
    "Could we just use every characters available in the keyboard? No, this loses relational meaning within words (i.e. \"language\" is more meaningful than \"gangeula\")\n",
    "\n",
    "The most common practice is called **Byte-Pair encodings**. This solves the above two questions by providing us with a general way of splitting langague that is also efficient. However, it far from a perfect system as it is the source of many bugs (i.e. being bad at counting). \n",
    "\n",
    "High-Level algorithm:\n",
    "1. Start with a inital vocabulary of all individual characters as tokens\n",
    "2. Find the most common pair of tokens in the text, merge this pair into a new token, and re-tokenize the text with the new token\n",
    "3. Repeat step 2 until you reach a desired vocabulary size or no more pairs can be merged\n",
    "\n",
    "<details>\n",
    "<summary>Note: Space (\" \") counts as a character and therefore merges with space are very common</summary>\n",
    "\n",
    "```python\n",
    "import tiktoken \n",
    "\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "print(tokenizer.encode(\" a\")) # [257]\n",
    "print(tokenizer.encode(\"a\")) # [64]\n",
    "print(tokenizer.encode(\"a \")) # [64, 220]\n",
    "print(tokenizer.encode(\" i\")) # [1312]\n",
    "print(tokenizer.encode(\"i\")) # [72]\n",
    "print(tokenizer.encode(\"i \")) # [72]\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "### Converting tokens into vectors\n",
    "This process is pretty straight-forward. We can convert each token to a **one-hot encoding** of the vocabulary. **One-hot encoding** vectors are filled with zeros at ever position, except in the position corresponding to the token's index in the vocabulary. \n",
    "\n",
    "A key inuition about **one-hot encodings** is they allow you to think of each integer independently. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "t_i &= (0, \\dots, 0, 1, 0, \\dots, 0) \\quad \\text{is the one-hot encoding for the }i\\text{th token (length }d_{vocab}\\text{)} \\\\\n",
    "\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "<details>\n",
    "<summary>Not ideal things about tokenization</summary>\n",
    "\n",
    "**Capitalization and Leading spaces matter** \n",
    "\n",
    "```python\n",
    "import tiktoken \n",
    "\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "print(tokenizer.encode(\"Michael\")) # [13256]\n",
    "print(tokenizer.encode(\" Michael\")) # [3899]\n",
    "print(tokenizer.encode(\" michael\")) # [285, 40302]\n",
    "print(tokenizer.encode(\"michael\")) # [76, 40302]\n",
    "```\n",
    "\n",
    "**Arithmetic does not sense**\n",
    "Common numbers are bundle together.\n",
    "\n",
    "```python\n",
    "import tiktoken \n",
    "\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "print(tokenizer.encode(\"56873+3184623=123456789-1000000000\")) # [49211, 4790, 10, 36042, 3510, 1954, 28, 10163, 2231, 3134, 4531, 12, 16, 10535, 830]\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "In this notebook, we will not be implementing the tokenizer from scatch. Instead we will be importing OpenAI's [tiktoken](https://github.com/openai/tiktoken) library to use their offical tokenizer. \n",
    "\n",
    "To see a full walkthrough of implementing a tokenizer check out Karthapthy's video: [Let's build the GPT Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference text: A day without laughter is a day\n",
      "Tokenized sequence: [32, 1110, 1231, 20263, 318, 257, 1110]\n",
      "Reconstructed reference text: A day without laughter is a day\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "reference_text = \"A day without laughter is a day\" # \"A day without laughter is a day wasted\" - Charlie Chapin\n",
    "print(\"Reference text: \" + reference_text)\n",
    "\n",
    "tokens = tokenizer.encode(reference_text)\n",
    "print(\"Tokenized sequence: \" + str(tokens))\n",
    "\n",
    "reconstructed_reference_text = tokenizer.decode(tokens)\n",
    "print(\"Reconstructed reference text: \" + reconstructed_reference_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    d_model: int = 768\n",
    "    debug: bool = True\n",
    "    layer_norm_eps: float = 1e-5\n",
    "    d_vocab: int = 50257\n",
    "    init_range: float = 0.02\n",
    "    n_ctx: int = 1024\n",
    "    d_head: int = 64\n",
    "    d_mlp: int = 3072\n",
    "    n_heads: int = 12\n",
    "    n_layers: int = 12\n",
    "\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "embed.W_E          (50257, 768)\n",
      "pos_embed.W_pos    (1024, 768)\n",
      "blocks.0.ln1.w     (768,)\n",
      "blocks.0.ln1.b     (768,)\n",
      "blocks.0.ln2.w     (768,)\n",
      "blocks.0.ln2.b     (768,)\n",
      "blocks.0.attn.W_Q  (12, 768, 64)\n",
      "blocks.0.attn.W_O  (12, 64, 768)\n",
      "blocks.0.attn.b_Q  (12, 64)\n",
      "blocks.0.attn.b_O  (768,)\n",
      "blocks.0.attn.W_K  (12, 768, 64)\n",
      "blocks.0.attn.W_V  (12, 768, 64)\n",
      "blocks.0.attn.b_K  (12, 64)\n",
      "blocks.0.attn.b_V  (12, 64)\n",
      "blocks.0.mlp.W_in  (768, 3072)\n",
      "blocks.0.mlp.b_in  (3072,)\n",
      "blocks.0.mlp.W_out (3072, 768)\n",
      "blocks.0.mlp.b_out (768,)\n",
      "ln_final.w         (768,)\n",
      "ln_final.b         (768,)\n",
      "unembed.W_U        (768, 50257)\n",
      "unembed.b_U        (50257,)\n"
     ]
    }
   ],
   "source": [
    "reference_gpt2 = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    fold_ln=False,\n",
    "    center_unembed=False,\n",
    "    center_writing_weights=False,  # you'll learn about these arguments later!\n",
    ")\n",
    "\n",
    "for name, param in reference_gpt2.named_parameters():\n",
    "    # Only print for first layer\n",
    "    if \".0.\" in name or \"blocks\" not in name:\n",
    "        print(f\"{name:18} {tuple(param.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding\n",
    "## Token Embedding\n",
    "\n",
    "Simply lookup table from token to embedding vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embed(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_E = nn.Parameter(t.empty((cfg.d_vocab, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_E, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(self, tokens: Int[Tensor, \"batch position\"]) -> Float[Tensor, \"batch position d_model\"]:\n",
    "        return self.W_E[tokens]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Embedding\n",
    "\n",
    "Similar to token embeddings, positional embeddings are a lookup table where the indices are simply the positions (0, 1, 2, ...) of tokens in the sequence rather than token IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEmbed(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_pos = nn.Parameter(t.empty((cfg.n_ctx, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_pos, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(self, tokens: Int[Tensor, \"batch position\"]) -> Float[Tensor, \"batch position d_model\"]:\n",
    "        batch, seq_len = tokens.shape\n",
    "        return einops.repeat(self.W_pos[:seq_len], \"seq d_model -> batch seq d_model\", batch=batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Norm\n",
    "\n",
    "Layer normalization is a technique used to stabilize the learning process in deep neural networks. It normalizes the input to each layer by subtracting the mean and dividing by the standard deviation, which helps in stabilizing the gradients and preventing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.w = nn.Parameter(t.ones(cfg.d_model))\n",
    "        self.b = nn.Parameter(t.zeros(cfg.d_model))\n",
    "\n",
    "    def forward(self, residual: Float[Tensor, \"batch posn d_model\"]) -> Float[Tensor, \"batch posn d_model\"]:\n",
    "        residual_mean = residual.mean(dim=-1, keepdim=True)\n",
    "        residual_std = (residual.var(dim=-1, keepdim=True, unbiased=False) + self.cfg.layer_norm_eps).sqrt()\n",
    "\n",
    "        residual = (residual - residual_mean) / residual_std\n",
    "        return residual * self.w + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention\n",
    "\n",
    "## Causal Mask\n",
    "\n",
    "The causal mask ensures that each position in the sequence can only attend to previous positions and itself.\n",
    "This is crucial for maintaining the autoregressive property during training and inference.\n",
    "\n",
    "For example, when predicting the 3rd token, the model should only look at tokens 1 and 2,\n",
    "not tokens 4 and beyond which would leak information from the future.\n",
    "\n",
    "The mask is implemented as a triangular matrix where:\n",
    "- The diagonal and lower triangle contain 1's (allowing attention)\n",
    "- The upper triangle contains 0's (blocking attention)\n",
    "\n",
    "During attention score calculation, the 0's are converted to negative infinity,\n",
    "which become 0 after the softmax operation, effectively preventing attention to future tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    IGNORE: Float[Tensor, \"\"]\n",
    "\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.register_buffer(\"IGNORE\", t.tensor(float(\"-inf\"), dtype=t.float32, device=device))\n",
    "\n",
    "    def apply_causal_mask(\n",
    "        self,\n",
    "        attn_scores: Float[Tensor, \"batch n_heads query_pos key_pos\"],\n",
    "    ) -> Float[Tensor, \"batch n_heads query_pos key_pos\"]:\n",
    "        \"\"\"\n",
    "        Applies a causal mask to attention scores, and returns masked scores.\n",
    "        \"\"\"\n",
    "        # Define a mask that is True for all positions we want to set probabilities to zero for\n",
    "        all_ones = t.ones(attn_scores.size(-2), attn_scores.size(-1), device=attn_scores.device)\n",
    "        mask = t.triu(all_ones, diagonal=1).bool()\n",
    "        # Apply the mask to attention scores, then return the masked scores\n",
    "        attn_scores.masked_fill_(mask, self.IGNORE)\n",
    "        return attn_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Heads\n",
    "\n",
    "Each attention head performs the following steps:\n",
    "\n",
    "1. Create attention pattern:\n",
    "   - Map input to queries and keys (shape: [batch, seq_pos, head_idx, d_head])\n",
    "   - Compute attention scores by taking dot product of queries and keys\n",
    "   - Scale scores by dividing by sqrt(d_head) to prevent vanishing gradients\n",
    "   - Apply causal mask to ensure tokens only attend to past/present\n",
    "   - Apply softmax to get attention probabilities\n",
    "\n",
    "2. Use attention pattern to aggregate information:\n",
    "   - Map input to values (shape: [batch, seq_pos, head_idx, d_head]) \n",
    "   - Weight and sum values according to attention probabilities\n",
    "   - Combine results across heads to get final output (shape: [batch, seq_pos, d_model])\n",
    "\n",
    "The attention mechanism allows each token to dynamically focus on relevant past tokens,\n",
    "with the scaling and masking ensuring stable training and causality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-88723190-1a80\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, AttentionPatterns } from \"https://unpkg.com/circuitsvis@1.43.3/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-88723190-1a80\",\n",
       "      AttentionPatterns,\n",
       "      {\"tokens\": [\"<|endoftext|>\", \"I\", \" am\", \" a\", \" powerful\", \" language\", \" model\", \" trained\", \" on\", \" massive\", \" amounts\", \" of\", \" text\", \" data\", \".\", \" Soon\", \" I\", \" will\", \" learn\", \" to\", \" understand\", \" and\", \" generate\", \" human\", \"-\", \"like\", \" content\", \"!\"], \"attention\": [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9679255485534668, 0.03207442909479141, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8024235963821411, 0.16839201748371124, 0.02918434888124466, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.720842182636261, 0.11982918530702591, 0.13272273540496826, 0.026605894789099693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5321496725082397, 0.13977733254432678, 0.11033761501312256, 0.09859553724527359, 0.11913977563381195, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6143989562988281, 0.09069888293743134, 0.09029288589954376, 0.1025661751627922, 0.06082509085536003, 0.04121799021959305, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3596109449863434, 0.08822927623987198, 0.07283925265073776, 0.10122088342905045, 0.0638948604464531, 0.11232010275125504, 0.2018847018480301, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.30887359380722046, 0.08113713562488556, 0.06460878998041153, 0.03843513876199722, 0.06786850094795227, 0.07331458479166031, 0.19652175903320312, 0.16924050450325012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.370820015668869, 0.09825066477060318, 0.12316661328077316, 0.03591048717498779, 0.07460278272628784, 0.06773842871189117, 0.06624216586351395, 0.14241650700569153, 0.020852357149124146, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.20529969036579132, 0.04844466969370842, 0.10717175155878067, 0.040879834443330765, 0.20097693800926208, 0.06507955491542816, 0.06263022124767303, 0.17241796851158142, 0.03641713783144951, 0.06068224087357521, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3479514420032501, 0.04595329239964485, 0.06891313940286636, 0.045302219688892365, 0.052851561456918716, 0.0733170211315155, 0.09569913148880005, 0.12846331298351288, 0.027184171602129936, 0.06415703892707825, 0.05020766332745552, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.32389652729034424, 0.07083306461572647, 0.08319319039583206, 0.023974770680069923, 0.0798560306429863, 0.032999392598867416, 0.05343625694513321, 0.10479963570833206, 0.019253142178058624, 0.1247246116399765, 0.06603152304887772, 0.017001882195472717, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3222404420375824, 0.034752193838357925, 0.03427283838391304, 0.04232526943087578, 0.06074235215783119, 0.1299091875553131, 0.035506561398506165, 0.11902541667222977, 0.0265328511595726, 0.03529689460992813, 0.02719612419605255, 0.06447140872478485, 0.06772838532924652, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1708398461341858, 0.021353818476200104, 0.0689564198255539, 0.019581817090511322, 0.04683532565832138, 0.16439688205718994, 0.03833505138754845, 0.29747921228408813, 0.015617323108017445, 0.026523396372795105, 0.0375906340777874, 0.021440420299768448, 0.04572755843400955, 0.02532225102186203, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4612792134284973, 0.06182016059756279, 0.05226418748497963, 0.014494244940578938, 0.05568237975239754, 0.0367693193256855, 0.01886160299181938, 0.06970533728599548, 0.00743261631578207, 0.06602528691291809, 0.03894670680165291, 0.0066828313283622265, 0.08306200057268143, 0.019637495279312134, 0.007336651906371117, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1764410436153412, 0.07948179543018341, 0.04946928471326828, 0.03997042775154114, 0.05845773220062256, 0.08346983045339584, 0.044216644018888474, 0.05640542134642601, 0.031266454607248306, 0.09799187630414963, 0.06573432683944702, 0.045266591012477875, 0.01963602378964424, 0.006997033953666687, 0.029563967138528824, 0.11563152819871902, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.30584287643432617, 0.045708637684583664, 0.026465529575943947, 0.006978639401495457, 0.029126813635230064, 0.023510737344622612, 0.027932636439800262, 0.018212804570794106, 0.007616949267685413, 0.03297735005617142, 0.03141546621918678, 0.00917391199618578, 0.025590956211090088, 0.015894096344709396, 0.008340608328580856, 0.34329840540885925, 0.04191352054476738, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.17280924320220947, 0.04234824702143669, 0.03848467394709587, 0.009374765679240227, 0.04072163999080658, 0.020013416185975075, 0.03101828694343567, 0.0792500227689743, 0.007135235238820314, 0.08468055725097656, 0.05235195532441139, 0.014073432423174381, 0.07139196246862411, 0.015591824427247047, 0.016652459278702736, 0.20445944368839264, 0.06740442663431168, 0.03223827853798866, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.18246690928936005, 0.029026489704847336, 0.017965098842978477, 0.02130134217441082, 0.05442141741514206, 0.05864645913243294, 0.0677078366279602, 0.16745807230472565, 0.014403160661458969, 0.03185473382472992, 0.021148251369595528, 0.02092134952545166, 0.026050187647342682, 0.022771073505282402, 0.017401622608304024, 0.15455909073352814, 0.0291141327470541, 0.03184772655367851, 0.030935050919651985, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.16924342513084412, 0.04902977868914604, 0.04953635111451149, 0.009600549004971981, 0.03539557009935379, 0.022780392318964005, 0.025169160217046738, 0.056528471410274506, 0.008438666351139545, 0.04860948771238327, 0.03309860825538635, 0.011249291710555553, 0.04998220503330231, 0.012874022126197815, 0.010496187955141068, 0.20278885960578918, 0.06837419420480728, 0.06050534173846245, 0.06700441241264343, 0.00929503794759512, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.20340469479560852, 0.053805090487003326, 0.017334919422864914, 0.014901138842105865, 0.03444596379995346, 0.04589380696415901, 0.031226424500346184, 0.06497292220592499, 0.016976619139313698, 0.04588228464126587, 0.03747745603322983, 0.01704035885632038, 0.027279363945126534, 0.026137733832001686, 0.01774490997195244, 0.1490989774465561, 0.06201699748635292, 0.03090503066778183, 0.04663175716996193, 0.021877435967326164, 0.03494613617658615, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.13575692474842072, 0.033832330256700516, 0.026070663705468178, 0.008460735902190208, 0.033062275499105453, 0.022669201716780663, 0.014488164335489273, 0.06414566189050674, 0.005358177702873945, 0.04423977807164192, 0.014018131420016289, 0.005897068418562412, 0.031902242451906204, 0.01074028480798006, 0.007713261526077986, 0.2679329216480255, 0.06677400320768356, 0.03253938630223274, 0.05760522559285164, 0.008323554880917072, 0.09891194850206375, 0.009558015502989292, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.16440008580684662, 0.02511792629957199, 0.012174125760793686, 0.016062963753938675, 0.03631570190191269, 0.05105084180831909, 0.054179783910512924, 0.07402684539556503, 0.01638639345765114, 0.029422800987958908, 0.04229066148400307, 0.028866104781627655, 0.04861614108085632, 0.07636018842458725, 0.016419611871242523, 0.033869799226522446, 0.020890673622488976, 0.01621725596487522, 0.06607592850923538, 0.0162370465695858, 0.04787445068359375, 0.026654860004782677, 0.08048985153436661, 0.0, 0.0, 0.0, 0.0, 0.0], [0.08467910438776016, 0.027965456247329712, 0.023711534217000008, 0.025655904784798622, 0.02385195530951023, 0.042281873524188995, 0.12542074918746948, 0.060055386275053024, 0.024011394008994102, 0.03877861425280571, 0.015526978299021721, 0.016689343377947807, 0.05737398564815521, 0.07095181196928024, 0.022362932562828064, 0.07735247910022736, 0.021274352446198463, 0.021437622606754303, 0.025994129478931427, 0.0243414044380188, 0.03768039867281914, 0.03207026794552803, 0.06709259003400803, 0.03343966230750084, 0.0, 0.0, 0.0, 0.0], [0.17847438156604767, 0.02420591562986374, 0.0491710901260376, 0.01691771298646927, 0.026987064629793167, 0.01796334981918335, 0.012055954895913601, 0.04066283628344536, 0.005846056621521711, 0.04810592532157898, 0.009814741089940071, 0.006304546259343624, 0.07642862200737, 0.0048114885576069355, 0.010912466794252396, 0.11419151723384857, 0.06097869575023651, 0.03155858442187309, 0.05476842075586319, 0.007593656424432993, 0.0851774588227272, 0.011237654834985733, 0.0703355148434639, 0.03030700795352459, 0.005189342889934778, 0.0, 0.0, 0.0], [0.10488392412662506, 0.033881817013025284, 0.028208844363689423, 0.019769970327615738, 0.039280831813812256, 0.02957218699157238, 0.023928094655275345, 0.05135248228907585, 0.017468184232711792, 0.027005651965737343, 0.01495455577969551, 0.023714203387498856, 0.04033282399177551, 0.016847457736730576, 0.025437230244278908, 0.10795579105615616, 0.028584210202097893, 0.03217707946896553, 0.046312060207128525, 0.02336570993065834, 0.08102215081453323, 0.029899321496486664, 0.05918547883629799, 0.02279684506356716, 0.029046563431620598, 0.043016474694013596, 0.0, 0.0], [0.09456035494804382, 0.01604861207306385, 0.017935940995812416, 0.005487713031470776, 0.03252993896603584, 0.062047671526670456, 0.01565474271774292, 0.03327503055334091, 0.006304719019681215, 0.025112802162766457, 0.0076794615015387535, 0.006873297970741987, 0.15901845693588257, 0.012292544357478619, 0.007290112320333719, 0.08165770769119263, 0.012861806899309158, 0.015212925150990486, 0.031632281839847565, 0.010230968706309795, 0.09784132987260818, 0.010681403800845146, 0.08340924233198166, 0.07929950207471848, 0.015279233455657959, 0.0253496952354908, 0.03443244844675064, 0.0], [0.1428237110376358, 0.023528603836894035, 0.016511954367160797, 0.006581771187484264, 0.024070119485259056, 0.026026736944913864, 0.010325761511921883, 0.022537827491760254, 0.006243005394935608, 0.02433134987950325, 0.03272748738527298, 0.004231088329106569, 0.039642442017793655, 0.008015524595975876, 0.0067056287080049515, 0.14099203050136566, 0.039838265627622604, 0.03656553849577904, 0.013807510957121849, 0.007199242245405912, 0.06446709483861923, 0.01342909224331379, 0.08088322728872299, 0.03160873055458069, 0.02041235938668251, 0.05569015443325043, 0.07869082689285278, 0.022112922742962837]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0004189967585261911, 0.9995810389518738, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00013394774578046054, 0.009511834010481834, 0.9903541803359985, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.002669971203431487, 0.006137894932180643, 0.019486667588353157, 0.9717054963111877, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [6.952325929887593e-05, 0.0008559343405067921, 0.004351260606199503, 0.0003442387969698757, 0.9943791031837463, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0004820493049919605, 0.03456844761967659, 0.0436975471675396, 0.001882773358374834, 0.0031173850875347853, 0.9162518382072449, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00017181903240270913, 0.0027766891289502382, 0.0006217768532224, 7.691870268899947e-05, 7.560408994322643e-05, 0.0003909334191121161, 0.9958862662315369, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [2.0425120965228416e-05, 0.00066713010892272, 0.000925199594348669, 0.0002293782599736005, 0.00020519511599559337, 0.0002979472919832915, 0.0009089193772524595, 0.9967458248138428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0008101458079181612, 0.00035102773108519614, 0.00035323522752150893, 0.0010845799697563052, 2.4684804884600453e-05, 6.439237949962262e-06, 1.866335514932871e-05, 3.341695628478192e-05, 0.9973178505897522, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.4049361197976395e-05, 0.0002555052924435586, 0.00034013469121418893, 0.00037998901098035276, 0.009196111932396889, 0.0001143210320151411, 0.00013841768668498844, 0.00015661594807170331, 6.425414903787896e-05, 0.9893407225608826, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [8.92568496055901e-05, 0.0008863834082148969, 0.0018186833476647735, 0.0007829000242054462, 0.00033732791780494153, 0.0001902169460663572, 9.331379260402173e-05, 0.00010220766125712544, 0.000532130419742316, 0.00282542803324759, 0.9923421740531921, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0012158507015556097, 0.0020776400342583656, 0.0021344595588743687, 0.00050870276754722, 0.0003645361284725368, 4.669946065405384e-05, 0.00012070932280039415, 0.00030341229285113513, 0.030741481110453606, 0.00018194643780589104, 0.0007606546860188246, 0.9615439176559448, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0003468642826192081, 0.004778317175805569, 0.001035067718476057, 0.00013275223318487406, 0.00011136284592794254, 0.014881034381687641, 0.00013828568626195192, 0.0002116686082445085, 0.00012554149725474417, 8.184355101548135e-05, 0.0011701877228915691, 6.310395838227123e-05, 0.9769241213798523, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0002619900042191148, 0.0009083801996894181, 0.00038797067827545106, 0.00027580445748753846, 0.00015816731320228428, 0.00032432968146167696, 0.0010939239291474223, 0.0003339485265314579, 0.0005733834696002305, 0.000495322048664093, 0.004560825880616903, 0.0006959401653148234, 0.0010991186136379838, 0.9888309836387634, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.011556918732821941, 0.0015527770156040788, 0.00014874331827741116, 0.00035143233253620565, 2.8756912797689438e-05, 2.133765519829467e-05, 0.00014076569641474634, 9.01708408491686e-05, 0.003111765021458268, 2.9955830541439354e-05, 0.0001264462189283222, 0.002362751169130206, 0.00021151595865376294, 5.326786413206719e-05, 0.9802133440971375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.4654498954769224e-05, 6.641871004831046e-05, 6.488166400231421e-05, 3.336517693242058e-05, 5.8111094404011965e-05, 6.817419034632621e-06, 3.1513279736827826e-06, 7.635103429493029e-06, 5.063481512479484e-05, 0.0004143792612012476, 2.1078501958982088e-05, 1.9085473468294367e-05, 6.399045560101513e-07, 6.861256224510726e-06, 1.86578108696267e-06, 0.9992305040359497, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0016652675112709403, 0.2677539885044098, 0.025643102824687958, 0.0009346284787170589, 3.895104237017222e-05, 0.0006791695486754179, 6.591220881091431e-05, 8.798520866548643e-05, 0.0001352320978185162, 0.00012778182281181216, 0.00030977767892181873, 0.0009053052053786814, 0.0004823351337108761, 0.0001454280863981694, 0.0009330796892754734, 0.0008402828825637698, 0.6992518305778503, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.000493442639708519, 0.00041840842459350824, 0.0010711470386013389, 2.9462597012752667e-05, 0.00013729272177442908, 2.047680754913017e-05, 0.0003476492129266262, 7.676742097828537e-05, 0.0011977421818301082, 8.520692790625617e-05, 0.000172417625435628, 0.0008990196511149406, 4.45956175099127e-06, 3.085481148445979e-05, 5.789405986433849e-05, 0.0019414351554587483, 0.00010800305608427152, 0.992908239364624, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00023187916667666286, 0.001131137483753264, 0.002295031677931547, 0.000299072009511292, 0.0014704535715281963, 0.0004869048425462097, 0.00020765721274074167, 0.04106706753373146, 3.393035149201751e-05, 0.000342339975759387, 0.00017953537462744862, 0.0004929723800159991, 0.00034870055969804525, 0.0008780286880210042, 3.9716673200018704e-05, 0.00032113827182911336, 0.0004263166047167033, 0.00046442419989034534, 0.9492835402488708, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.002387777203693986, 0.0010610060999169946, 0.00047586444998160005, 0.0011281927581876516, 0.00013317563571035862, 7.865947554819286e-05, 3.926286444766447e-05, 0.0001429837429895997, 0.04745590314269066, 5.217799844103865e-05, 0.0006963794003240764, 0.02339886873960495, 4.3145551899215207e-05, 7.727995398454368e-05, 0.0065896837040781975, 0.0006627346738241613, 0.00026520295068621635, 0.0002868427836801857, 0.00011221221211599186, 0.914912760257721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [3.7718597013736144e-05, 0.0028871626127511263, 0.0007568832952529192, 4.594546408043243e-05, 0.0016737350961193442, 0.0003817910037469119, 0.001254242262803018, 0.0015634109731763601, 6.600633241760079e-06, 0.00018849247135221958, 2.8229345844010822e-05, 0.00013129513536114246, 8.486991282552481e-05, 7.089754944900051e-05, 3.817645847448148e-05, 3.0261850042734295e-05, 0.0007363763870671391, 0.00016525141836609691, 0.0025027403607964516, 1.884853554656729e-05, 0.9873968958854675, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0008936197264119983, 0.002172689186409116, 0.0001693311642156914, 0.00023638214042875916, 2.2114210878498852e-05, 2.7180067263543606e-05, 0.0008545893942937255, 9.283175313612446e-05, 0.0053001828491687775, 3.8504986150655895e-05, 6.117831071605906e-05, 0.006428701803088188, 3.54762451024726e-05, 7.645203731954098e-05, 0.007992781698703766, 0.0002278560568811372, 0.0003495707642287016, 0.00023333783610723913, 1.7993512301472947e-05, 0.022607339546084404, 0.00018417552928440273, 0.951977550983429, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00022398418514057994, 0.0005094383959658444, 0.00014438368089031428, 0.0002894065692089498, 0.002682957099750638, 0.0007350408704951406, 0.0053161755204200745, 0.0004973278846591711, 4.053051088703796e-05, 0.0207965150475502, 0.0027041907887905836, 0.0003151462005916983, 0.00019006589718628675, 0.0012905487092211843, 2.111047797370702e-05, 0.00018159647879656404, 8.210979285649955e-05, 0.0005879673408344388, 0.000841478118672967, 0.00035062641836702824, 0.00046938666491769254, 0.00035183224827051163, 0.9613781571388245, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00010742104495875537, 0.0009582572965882719, 0.000882620457559824, 0.0006224631797522306, 0.0011516218073666096, 0.0005830228328704834, 0.00027258446789346635, 0.002074422314763069, 2.3318396415561438e-05, 0.0016818203730508685, 5.6964443501783535e-05, 9.906713967211545e-05, 5.201328167458996e-05, 0.00010610496974550188, 5.7187779020750895e-05, 6.0475758800748736e-05, 0.000500147754792124, 2.2666908989776857e-05, 0.00013418951130006462, 6.256021879380569e-05, 0.0007466272218152881, 4.642425483325496e-05, 0.00010860885231522843, 0.989589273929596, 0.0, 0.0, 0.0, 0.0], [0.0007441395428031683, 0.0006771553307771683, 0.0006384465377777815, 9.909349319059402e-05, 0.0001028805854730308, 5.694407082046382e-05, 0.00010265223681926727, 0.0004054458695463836, 0.000474768690764904, 3.9446302253054455e-05, 0.00014668509538751096, 0.003414642997086048, 2.9834041924914345e-05, 4.129638182348572e-05, 0.0016989671858027577, 0.00012996932491660118, 7.25727149983868e-05, 0.00017625847249291837, 6.422955630114302e-05, 0.0035963139962404966, 6.793873035348952e-05, 0.0030953933019191027, 1.2928376236231998e-05, 0.0001333181862719357, 0.9839786887168884, 0.0, 0.0, 0.0], [0.0001739672152325511, 0.001044382806867361, 0.0013302534352988005, 0.001810557208955288, 0.00109164381865412, 0.0003735946665983647, 0.0009874147363007069, 0.000445191515609622, 0.0004711482615675777, 0.0009150975965894759, 0.00027112371753901243, 0.0016056320164352655, 9.864033199846745e-05, 6.97946161380969e-05, 0.00021863308211322874, 0.0002980470599140972, 0.00047436781460419297, 0.0001422002533217892, 0.0002003055124077946, 0.0014994473895058036, 0.0010134357726201415, 0.005611407570540905, 0.0009418324334546924, 0.000910412345547229, 0.0002390190784353763, 0.9777625203132629, 0.0, 0.0], [0.0003527826047502458, 0.00044714947580359876, 0.0022515342570841312, 0.0005239044548943639, 0.0016068968689069152, 0.0016214586794376373, 0.0008273706771433353, 0.00024545297492295504, 0.000781338894739747, 0.0011672599939629436, 0.011300666257739067, 0.00030561856692656875, 0.025701377540826797, 0.0031005179043859243, 5.991096259094775e-05, 0.00028036042931489646, 0.0001156554208137095, 0.00013212597696110606, 0.00019487680401653051, 0.00017459967057220638, 0.0013355365954339504, 0.0005400251247920096, 0.0005865146522410214, 0.00043235308839939535, 3.6784549592994153e-05, 0.0007466780371032655, 0.945131242275238, 0.0], [0.0019696280360221863, 0.0003170048294123262, 0.00011121921124868095, 6.217061581992311e-06, 2.136849070666358e-05, 7.378958798653912e-06, 1.7180007489514537e-05, 2.7184754799236543e-05, 0.0002805351105052978, 1.2325382158451248e-05, 7.4869989475701e-05, 0.0001387483935104683, 5.574819624598604e-06, 4.911341420665849e-06, 0.0013210815377533436, 0.001574546447955072, 0.00012630436685867608, 9.616907482268289e-05, 0.00015296800120268017, 0.00016292554209940135, 6.1120925238356e-05, 8.445205457974225e-05, 2.9774693757644854e-05, 3.67280408681836e-05, 3.68838882423006e-05, 0.00012783284182660282, 4.4308264477876946e-05, 0.9931507110595703]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9424744844436646, 0.05752556398510933, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8506750464439392, 0.09740974009037018, 0.0519152469933033, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7708350419998169, 0.10665775835514069, 0.07756198942661285, 0.04494517296552658, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7078003287315369, 0.09608786553144455, 0.04335510730743408, 0.09233181923627853, 0.06042490899562836, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7200961112976074, 0.09158550202846527, 0.05549364537000656, 0.057972922921180725, 0.02373611554503441, 0.0511157251894474, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6023576259613037, 0.08661676943302155, 0.0652889758348465, 0.07414914667606354, 0.027819709852337837, 0.10835905373096466, 0.035408686846494675, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5271425843238831, 0.0840296596288681, 0.04464827850461006, 0.07474824041128159, 0.03431031480431557, 0.16239754855632782, 0.028414390981197357, 0.04430902376770973, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.40498992800712585, 0.11759874224662781, 0.09521756321191788, 0.057437963783741, 0.07323752343654633, 0.04887155443429947, 0.03770994767546654, 0.04001597315073013, 0.12492089718580246, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.42801666259765625, 0.07641341537237167, 0.06518237292766571, 0.07724928855895996, 0.07745998352766037, 0.09069778770208359, 0.045975252985954285, 0.04888354241847992, 0.056600846350193024, 0.03352085500955582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.48769253492355347, 0.07243329286575317, 0.04473661631345749, 0.05550668016076088, 0.07290378957986832, 0.06450241804122925, 0.055848006159067154, 0.029862189665436745, 0.05529781058430672, 0.04156138375401497, 0.019655266776680946, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3489549160003662, 0.06830856949090958, 0.04691818356513977, 0.04186610132455826, 0.0750066414475441, 0.047706685960292816, 0.03277523070573807, 0.0659332349896431, 0.1421598345041275, 0.03578319400548935, 0.03214537724852562, 0.062442101538181305, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.36769556999206543, 0.1021031066775322, 0.05668316036462784, 0.04148304462432861, 0.029980730265378952, 0.19534006714820862, 0.03518177196383476, 0.028986502438783646, 0.027342554181814194, 0.016985559836030006, 0.02725367434322834, 0.027340449392795563, 0.04362374171614647, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3576436936855316, 0.05850297585129738, 0.05349640175700188, 0.04702625423669815, 0.03410579264163971, 0.05063600093126297, 0.033398427069187164, 0.08265645802021027, 0.03954719007015228, 0.03233897686004639, 0.08061270415782928, 0.043449316173791885, 0.0312793105840683, 0.05530649051070213, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3331831395626068, 0.2012568861246109, 0.08783542364835739, 0.016641447320580482, 0.02456020563840866, 0.01998058706521988, 0.027446268126368523, 0.03277897834777832, 0.04075218364596367, 0.019287675619125366, 0.03756270930171013, 0.07709740102291107, 0.021419769152998924, 0.020009709522128105, 0.040187545120716095, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.32036060094833374, 0.04726642742753029, 0.029456425458192825, 0.04551197588443756, 0.045982614159584045, 0.014866705983877182, 0.02571362629532814, 0.02539896033704281, 0.07331304997205734, 0.05670648440718651, 0.030657898634672165, 0.06882242113351822, 0.02666771411895752, 0.04505310207605362, 0.08307043462991714, 0.06115156039595604, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.29514217376708984, 0.05528173968195915, 0.05968980863690376, 0.02388695627450943, 0.029750855639576912, 0.030653569847345352, 0.020751137286424637, 0.04104180261492729, 0.048232149332761765, 0.02262275293469429, 0.03364204987883568, 0.07706105709075928, 0.022835062816739082, 0.0415201261639595, 0.08438768237829208, 0.06958027184009552, 0.04392069950699806, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2771981656551361, 0.04911443963646889, 0.04382583871483803, 0.022962423041462898, 0.037257105112075806, 0.020309312269091606, 0.0233832485973835, 0.032787203788757324, 0.05383912846446037, 0.026608318090438843, 0.04931570962071419, 0.05854891613125801, 0.03333623334765434, 0.03719305992126465, 0.06991057097911835, 0.05121385306119919, 0.06036154553294182, 0.05283491685986519, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2961001694202423, 0.046709515154361725, 0.03674845024943352, 0.03211333602666855, 0.03961293399333954, 0.0385049432516098, 0.03433982655405998, 0.01904742419719696, 0.04441127926111221, 0.03216695412993431, 0.020900126546621323, 0.04873169586062431, 0.0458032600581646, 0.044754765927791595, 0.04444754496216774, 0.07876396924257278, 0.039994239807128906, 0.04199584200978279, 0.014853655360639095, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1791113168001175, 0.02514943666756153, 0.03235844895243645, 0.013551175594329834, 0.031810298562049866, 0.015301954001188278, 0.02241828665137291, 0.03261769935488701, 0.06730271875858307, 0.030113602057099342, 0.025431862100958824, 0.05695318430662155, 0.020835263654589653, 0.03312642499804497, 0.1532345414161682, 0.0547507107257843, 0.024863267317414284, 0.04485713690519333, 0.03547458350658417, 0.10073797404766083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2550051808357239, 0.03417613357305527, 0.04123414680361748, 0.028062719851732254, 0.028177518397569656, 0.021158622577786446, 0.04948066174983978, 0.0359836146235466, 0.03625345230102539, 0.0196541715413332, 0.039783380925655365, 0.04359327256679535, 0.0176100917160511, 0.03717629984021187, 0.08527471870183945, 0.0839913859963417, 0.0369267575442791, 0.03223881497979164, 0.022404585033655167, 0.03450924903154373, 0.017305204644799232, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.13686296343803406, 0.01746569387614727, 0.019335361197590828, 0.01042849663645029, 0.01205766573548317, 0.0134818060323596, 0.014918377622961998, 0.03173208609223366, 0.03339652344584465, 0.014415633864700794, 0.013807418756186962, 0.03965191915631294, 0.015658816322684288, 0.016713105142116547, 0.12680888175964355, 0.01701957732439041, 0.016821304336190224, 0.02303108386695385, 0.018965857103466988, 0.05623199790716171, 0.02575025148689747, 0.32544514536857605, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.23023493587970734, 0.031704213470220566, 0.03382300212979317, 0.03303930163383484, 0.01805083267390728, 0.03286457806825638, 0.024866824969649315, 0.011444511823356152, 0.03490147367119789, 0.022467784583568573, 0.028763076290488243, 0.03616868332028389, 0.1163335070014, 0.03312073275446892, 0.03608764708042145, 0.045783426612615585, 0.04945326969027519, 0.03426167741417885, 0.019250961020588875, 0.04308217018842697, 0.031012101098895073, 0.03857455402612686, 0.01471068523824215, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2750464677810669, 0.035494234412908554, 0.042563360184431076, 0.03276962414383888, 0.04461914673447609, 0.04014396667480469, 0.026996688917279243, 0.015583609230816364, 0.028754035010933876, 0.016053514555096626, 0.019665168598294258, 0.029569048434495926, 0.040877148509025574, 0.026645798236131668, 0.027920087799429893, 0.03704154118895531, 0.03150369971990585, 0.02784701995551586, 0.024402601644396782, 0.025395341217517853, 0.03392045944929123, 0.029050854966044426, 0.006772118620574474, 0.08136442303657532, 0.0, 0.0, 0.0, 0.0], [0.1954762488603592, 0.01800966076552868, 0.017570024356245995, 0.006657250691205263, 0.004876044578850269, 0.009343341924250126, 0.015949588268995285, 0.009785975329577923, 0.026402920484542847, 0.008499358780682087, 0.008038779720664024, 0.035086359828710556, 0.006307982839643955, 0.008607185445725918, 0.05281698703765869, 0.039301615208387375, 0.009824046865105629, 0.030186811462044716, 0.006654086988419294, 0.02672424726188183, 0.009572867304086685, 0.028458308428525925, 0.004699491895735264, 0.008404767140746117, 0.41274598240852356, 0.0, 0.0, 0.0], [0.22478458285331726, 0.022503262385725975, 0.027614613994956017, 0.015312913805246353, 0.02674822323024273, 0.025648288428783417, 0.03512753173708916, 0.037097278982400894, 0.04846053197979927, 0.013798236846923828, 0.010156518779695034, 0.038765788078308105, 0.022928988561034203, 0.044046614319086075, 0.04208902642130852, 0.06200195476412773, 0.011730262078344822, 0.027393579483032227, 0.028137536719441414, 0.04328538477420807, 0.03131125494837761, 0.02680176869034767, 0.015050245448946953, 0.03736879676580429, 0.04640156775712967, 0.03543524816632271, 0.0, 0.0], [0.25505051016807556, 0.022460123524069786, 0.03164678066968918, 0.02647438831627369, 0.09370561689138412, 0.03173854202032089, 0.017239391803741455, 0.03630003705620766, 0.010700765065848827, 0.020964911207556725, 0.024710845202207565, 0.023297244682908058, 0.023390071466565132, 0.02697397768497467, 0.01805218681693077, 0.08620987087488174, 0.02224534936249256, 0.01348117459565401, 0.0595388188958168, 0.01186998002231121, 0.023088719695806503, 0.013839490711688995, 0.012208263389766216, 0.016111353412270546, 0.01549938041716814, 0.010318581946194172, 0.05288364738225937, 0.0], [0.13045619428157806, 0.06112825870513916, 0.028125304728746414, 0.009642384015023708, 0.011039053089916706, 0.012621390633285046, 0.014829685911536217, 0.010874337516725063, 0.01900811493396759, 0.008636037819087505, 0.027992505580186844, 0.035746973007917404, 0.025084927678108215, 0.0258964654058218, 0.03631327673792839, 0.07996778935194016, 0.057302750647068024, 0.09389499574899673, 0.02486961893737316, 0.016354011371731758, 0.044631365686655045, 0.020586419850587845, 0.017720762640237808, 0.022513192147016525, 0.023187125101685524, 0.03409905731678009, 0.06197379156947136, 0.04550417512655258]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10777998715639114, 0.8922200202941895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.011847440153360367, 0.553614616394043, 0.4345378875732422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.13186009228229523, 0.04053741320967674, 0.13830414414405823, 0.6892983317375183, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0041991486214101315, 0.00015148821694310755, 0.0013802706962451339, 0.0017565644811838865, 0.9925124645233154, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.013250951655209064, 0.0011867963476106524, 0.012139358557760715, 0.0028686642181128263, 0.03879985585808754, 0.9317542910575867, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0018455666722729802, 0.00015052886737976223, 0.00016302552830893546, 0.00035809789551422, 0.009412990882992744, 0.01123977079987526, 0.976830005645752, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.004806131590157747, 0.00013222056441009045, 0.0006572756101377308, 7.979354995768517e-05, 0.006570640951395035, 0.004082298371940851, 0.01630493439733982, 0.967366635799408, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.011415338143706322, 0.0014832804445177317, 0.003301378805190325, 0.0021927838679403067, 0.013952433131635189, 0.005063753109425306, 0.013764471746981144, 0.18961551785469055, 0.7592110633850098, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0016170181334018707, 1.3951323126093484e-05, 4.1611572669353336e-05, 9.211369615513831e-05, 0.008315729908645153, 0.00032164977164939046, 0.002374139614403248, 0.010473706759512424, 0.003415119368582964, 0.9733350276947021, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0024776326026767492, 9.883064194582403e-06, 8.620552398497239e-05, 2.15351719816681e-05, 0.0012342939153313637, 0.00019505000091157854, 0.00018046471814159304, 0.003120017470791936, 0.000347196648363024, 0.11837804317474365, 0.8739497065544128, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0027322566602379084, 0.0004999689990654588, 0.00031543042859993875, 0.0011774651939049363, 0.003624389646574855, 0.003367895958945155, 0.029794784262776375, 0.01873866841197014, 0.047092050313949585, 0.05517986789345741, 0.3386572003364563, 0.49882006645202637, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.004891885910183191, 9.584568033460528e-05, 9.229571878677234e-05, 1.737149796099402e-05, 0.000856806815136224, 0.0027442891150712967, 0.00020486602443270385, 0.002932480536401272, 0.00023694879200775176, 0.003942771814763546, 0.011650060303509235, 0.0029355459846556187, 0.9693989157676697, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.009515752084553242, 6.137710442999378e-05, 6.560350448125973e-05, 5.7652901887195185e-05, 0.00046901105088181794, 0.0004906583344563842, 0.000802046328317374, 0.0017717196606099606, 0.0004772279062308371, 0.013935402035713196, 0.024379318580031395, 0.0028958339244127274, 0.1745324730873108, 0.7705458998680115, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.016143864020705223, 0.0007187096052803099, 0.0009041250450536609, 0.0006786067970097065, 0.002033487893640995, 0.0019097606418654323, 0.005149980075657368, 0.018976548686623573, 0.010220814496278763, 0.029290761798620224, 0.02532407082617283, 0.07007195800542831, 0.09316577017307281, 0.22538474202156067, 0.5000267624855042, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00028277665842324495, 1.1083518529630965e-06, 3.8286175367829856e-06, 2.2015640865902242e-07, 3.028128503501648e-06, 3.917370577255497e-06, 1.100996314562508e-06, 2.053275784419384e-05, 6.607225259358529e-06, 2.906835470639635e-05, 2.3757120288792066e-05, 2.5200244635925628e-05, 8.677206642460078e-05, 6.245089753065258e-05, 5.9450234402902424e-05, 0.9993902444839478, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00018606087542138994, 0.00024517724523320794, 0.00010853073035832494, 1.081763366528321e-05, 2.090770794893615e-05, 1.5882100342423655e-05, 3.204605673090555e-05, 0.0003328310267534107, 0.00013610790483653545, 0.0001528715220047161, 0.0003571062406990677, 0.001348781050182879, 0.000765207048971206, 0.0012519365409389138, 0.005631118081510067, 0.02030734159052372, 0.9690971970558167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0006013638339936733, 3.4070503716066014e-06, 2.4385574306506896e-06, 7.5767989073938224e-06, 1.0943499546556268e-05, 8.477523806504905e-06, 1.998773223021999e-05, 0.00020132293866481632, 6.850491627119482e-05, 0.0001800234749680385, 0.0002436625654809177, 0.0006624212837778032, 0.00016387150390073657, 0.0019568507559597492, 0.004942605271935463, 0.04539037123322487, 0.011086705140769482, 0.9344493746757507, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [3.698125874507241e-05, 1.1017071699370717e-07, 1.2836068208343931e-07, 1.327060914491085e-07, 7.632295933035493e-07, 2.502236213786091e-07, 6.940886123629753e-07, 1.2326197975198738e-05, 7.93462220372021e-07, 2.8698996175080538e-06, 1.2883837371191476e-05, 1.5100040400284342e-05, 2.291480268468149e-05, 4.331301533966325e-05, 1.967912794498261e-05, 0.002203571144491434, 0.0004224715521559119, 0.003060779767110944, 0.9941442608833313, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0012262621894478798, 9.55299947236199e-06, 1.2668456292885821e-05, 3.9158370782388374e-05, 1.7148071492556483e-05, 3.869167994707823e-05, 2.1805351934744976e-05, 3.678812936414033e-05, 0.0003227630804758519, 0.00016462862549815327, 0.010477244853973389, 0.0016469265101477504, 0.00042537375702522695, 0.0027057360857725143, 0.005923717748373747, 0.007090380880981684, 0.007976708933711052, 0.025048764422535896, 0.09808344393968582, 0.838732123374939, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.1485159120638855e-05, 6.656209450284223e-08, 6.18535196394987e-08, 5.1093554276349096e-08, 1.2322304598910705e-07, 1.1242742914419068e-07, 6.206319653756509e-07, 7.036790975689655e-06, 1.8077567176533194e-07, 1.4099455256655347e-06, 3.4928284549096134e-06, 2.9329873996175593e-06, 1.3587565263151191e-05, 9.958012014976703e-06, 7.199467290774919e-06, 0.000600290484726429, 0.00011427389836171642, 0.00121389573905617, 0.24326898157596588, 0.00025983815430663526, 0.7544844746589661, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.002549324417486787, 2.5992807422881015e-05, 1.8484734027879313e-05, 3.4334956581005827e-05, 8.475688446196727e-06, 1.2081554814358242e-05, 2.792035047605168e-05, 4.3166506657144055e-05, 8.813183376332745e-05, 6.861921428935602e-05, 0.00010281139111611992, 0.00045731657883152366, 0.00014046125579625368, 0.000506788317579776, 0.003544200211763382, 0.002673085080459714, 0.0054220701567828655, 0.0159150417894125, 0.010934743098914623, 0.07645723223686218, 0.06184140220284462, 0.8191283345222473, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00025078162434510887, 9.476482887293969e-08, 1.7872517332762072e-07, 2.3964770434758975e-07, 9.66587322182022e-07, 4.333032563863526e-07, 7.881543524490553e-07, 2.6283667011739453e-06, 1.0616078043312882e-06, 6.836010288679972e-05, 2.5921584892785177e-05, 4.463750883587636e-06, 2.3942955522215925e-05, 8.454614726360887e-05, 1.875229281722568e-05, 0.00011379802890587598, 2.1921790903434157e-05, 0.0004147291765548289, 0.00495385006070137, 0.0007049232954159379, 0.007684045471251011, 0.002241932088509202, 0.9833815097808838, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0005667427903972566, 2.175845708052293e-07, 4.2762832208609325e-07, 3.769371517137188e-07, 1.7060470781871118e-05, 1.1731137306014716e-07, 4.178354799933004e-07, 1.0298572306055576e-05, 3.410420106320089e-07, 1.916229484777432e-05, 1.8937304275823408e-06, 8.748485811338469e-07, 1.0283208666805876e-06, 2.674967163329711e-06, 3.3373628411936807e-06, 6.029450742062181e-05, 8.445585081062745e-06, 3.8643025618512183e-05, 6.296174979070202e-05, 0.0001274949318030849, 0.00015147315571084619, 0.0002327216643607244, 0.004501141607761383, 0.9941918253898621, 0.0, 0.0, 0.0, 0.0], [0.0011306251399219036, 1.7905907952808775e-05, 1.2489936125348322e-05, 2.18675904761767e-05, 3.604571975301951e-05, 8.572418664698489e-06, 6.490191299235448e-05, 9.198314364766702e-05, 4.3076779547845945e-05, 8.490086474921554e-05, 4.990146044292487e-05, 0.00016736469115130603, 0.0001067351913661696, 9.432417573407292e-05, 0.0010687574977055192, 0.0013897421304136515, 0.0011276326840743423, 0.002829504432156682, 0.003593434812501073, 0.01599389687180519, 0.005939784925431013, 0.05449769273400307, 0.024829423055052757, 0.2969856262207031, 0.5898138284683228, 0.0, 0.0, 0.0], [2.405797022220213e-05, 1.989042402783525e-06, 8.23882942313503e-07, 7.549063525402744e-07, 2.0954807951056864e-06, 5.807998604723252e-06, 6.135381681815488e-06, 3.239192665205337e-06, 6.989270673329884e-07, 9.970885912480298e-07, 7.32587359379977e-05, 1.0436537195346318e-05, 1.8146371075999923e-05, 8.599447028245777e-06, 2.543137270549778e-05, 3.83542210329324e-05, 0.00010492588626220822, 0.0003682460228446871, 0.0022908709943294525, 0.0005918138776905835, 0.017890801653265953, 0.0017259351443499327, 0.0029979445971548557, 0.020575186237692833, 0.017491810023784637, 0.9357416033744812, 0.0, 0.0], [0.0005361728835850954, 2.8588567602128023e-07, 4.038795680116891e-07, 1.009643071370192e-07, 1.8558201873020153e-06, 1.3542321539716795e-06, 1.0508598506930866e-06, 2.193231466662837e-06, 8.660413186589722e-08, 4.397498287289636e-06, 1.1480944522190839e-05, 2.6158195964853803e-07, 5.54025245946832e-05, 2.6971798433805816e-05, 1.615385713193973e-06, 1.0088817361975089e-05, 7.525195087509928e-06, 4.3997679313179106e-06, 8.7133223132696e-05, 4.57691348856315e-05, 7.302490848815069e-05, 5.23197631991934e-05, 0.0025677536614239216, 0.01021893322467804, 0.0005988975754007697, 0.009344683960080147, 0.9763458371162415, 0.0], [0.0006240850780159235, 5.255342330201529e-06, 3.247854010623996e-06, 4.493992491916288e-06, 3.966740223404486e-06, 2.61455170402769e-06, 3.4186666653113207e-06, 8.545358468836639e-06, 2.3796003461029613e-06, 6.2120147958921734e-06, 4.441344117367407e-06, 6.260820100578712e-06, 7.114059371815529e-06, 2.4826862500049174e-05, 4.730182627099566e-05, 0.0001278521667700261, 0.0001610239560250193, 0.0008984621963463724, 0.0005452484474517405, 0.0006063026958145201, 0.0023461661767214537, 0.0016909874975681305, 0.002403769874945283, 0.009298525750637054, 0.006209048442542553, 0.007402010727673769, 0.011376754380762577, 0.9561796188354492]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9238101243972778, 0.07618981599807739, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3758925497531891, 0.10050603002309799, 0.5236014127731323, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.39756864309310913, 0.07713737338781357, 0.30537018179893494, 0.21992377936840057, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07313580065965652, 0.00501059228554368, 0.034815721213817596, 0.01719842292368412, 0.8698394298553467, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10391805320978165, 0.005498788319528103, 0.023529116064310074, 0.022425299510359764, 0.5561625361442566, 0.288466215133667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.025125714018940926, 0.004354935139417648, 0.0027229846455156803, 0.012724352069199085, 0.10490616410970688, 0.034724827855825424, 0.8154410719871521, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00925452634692192, 0.0021644318476319313, 0.005085554905235767, 0.004478003364056349, 0.02282756008207798, 0.027204131707549095, 0.04222768917679787, 0.8867580890655518, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.06629202514886856, 0.006157906260341406, 0.035390641540288925, 0.016180504113435745, 0.15174171328544617, 0.02585432305932045, 0.11613103747367859, 0.4339848756790161, 0.1482669711112976, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04051858186721802, 0.0016199598321691155, 0.008162038400769234, 0.006599757820367813, 0.09004576504230499, 0.0070670247077941895, 0.028239967301487923, 0.032075755298137665, 0.051015861332416534, 0.7346553206443787, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.018469134345650673, 0.000739909359253943, 0.0056030466221272945, 0.0033162652980536222, 0.019673120230436325, 0.002716864226385951, 0.014768189750611782, 0.019233524799346924, 0.017567288130521774, 0.493180513381958, 0.4047321677207947, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.029051173478364944, 0.0039002022240310907, 0.0065723019652068615, 0.005558760371059179, 0.06546635180711746, 0.019198140129446983, 0.08525226265192032, 0.13187150657176971, 0.024414466693997383, 0.1650550365447998, 0.37470999360084534, 0.08894973993301392, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04587945714592934, 0.000885201501660049, 0.0017452315660193563, 0.0027125549968332052, 0.03169814497232437, 0.009259626269340515, 0.010769418440759182, 0.009673785418272018, 0.006998877506703138, 0.08033445477485657, 0.07697749882936478, 0.013540107756853104, 0.7095257043838501, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04865884408354759, 0.0016078307526186109, 0.002804814139381051, 0.004055950324982405, 0.03857894241809845, 0.0023627697955816984, 0.0054894727654755116, 0.013266745023429394, 0.014350328594446182, 0.052405234426259995, 0.018524933606386185, 0.024982875213027, 0.10690268874168396, 0.6660086512565613, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.019801637157797813, 0.007692243903875351, 0.004314162768423557, 0.007892649620771408, 0.019301462918519974, 0.0067948829382658005, 0.016717880964279175, 0.023353291675448418, 0.055102214217185974, 0.047944147139787674, 0.05887679010629654, 0.1285904049873352, 0.09443031251430511, 0.1550978273153305, 0.3540900647640228, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.012823721393942833, 0.0023639611899852753, 0.001568921492435038, 0.001017748611047864, 0.003075634827837348, 0.001767283072695136, 0.0017458857037127018, 0.002126690000295639, 0.004244934767484665, 0.008056127466261387, 0.008855066262185574, 0.010902820155024529, 0.008937293663620949, 0.016244255006313324, 0.014352051541209221, 0.9019174575805664, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.051757294684648514, 0.01331215351819992, 0.019059699028730392, 0.003816373646259308, 0.005640662275254726, 0.0048776655457913876, 0.007729093078523874, 0.008686084300279617, 0.01920834369957447, 0.029043549671769142, 0.05421287566423416, 0.06312958151102066, 0.045137811452150345, 0.07260480523109436, 0.09968473762273788, 0.2757107615470886, 0.22638855874538422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03450411930680275, 0.0018610971746966243, 0.001872035674750805, 0.004281339701265097, 0.010785569436848164, 0.001554911257699132, 0.004959991667419672, 0.013263357803225517, 0.013569527305662632, 0.018990758806467056, 0.023284683004021645, 0.027975602075457573, 0.013836943544447422, 0.07116347551345825, 0.021494848653674126, 0.05806005001068115, 0.041943665593862534, 0.6365980505943298, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.007704748772084713, 0.0012518116272985935, 0.0009877693373709917, 0.0009260165388695896, 0.004508486948907375, 0.0008039918029680848, 0.007762830704450607, 0.028581945225596428, 0.0032616588287055492, 0.008279843255877495, 0.00564031396061182, 0.015008939430117607, 0.006104825064539909, 0.01663004234433174, 0.015688717365264893, 0.1328202486038208, 0.029567252844572067, 0.15322180092334747, 0.5612486600875854, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.008915840648114681, 0.0005556942196562886, 0.0015198877081274986, 0.0015088360523805022, 0.008870748803019524, 0.0028958655893802643, 0.003448238829150796, 0.013822663575410843, 0.003463574219495058, 0.010246804915368557, 0.03608888015151024, 0.00610683998093009, 0.014502136968076229, 0.043935466557741165, 0.010501221753656864, 0.14579012989997864, 0.015189168974757195, 0.1726827770471573, 0.2721114456653595, 0.2278437614440918, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0056217145174741745, 0.0007761817541904747, 0.0009001783328130841, 0.000572999648284167, 0.001597089460119605, 0.0025393650867044926, 0.0051129646599292755, 0.0070672594010829926, 0.0011871905298903584, 0.0038927593268454075, 0.010769559070467949, 0.004370358772575855, 0.012829242274165154, 0.006166819017380476, 0.008386345580220222, 0.029199017211794853, 0.009652615524828434, 0.03201979026198387, 0.1571953296661377, 0.039742909371852875, 0.6604003310203552, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.007224656641483307, 0.0024746544659137726, 0.002320871688425541, 0.0027390869800001383, 0.0038054652977734804, 0.0018038632115349174, 0.003443838097155094, 0.008798276074230671, 0.0034502982161939144, 0.003131535369902849, 0.007148183416575193, 0.007616721093654633, 0.00786513090133667, 0.01015348732471466, 0.00627092132344842, 0.02668510563671589, 0.03495868295431137, 0.19904732704162598, 0.1362365037202835, 0.1026758924126625, 0.20709039270877838, 0.21505902707576752, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0105366799980402, 0.0003141565539408475, 0.00033098741550929844, 0.000193109386600554, 0.002610516967251897, 0.00022744147281628102, 0.00239795190282166, 0.0006188689148984849, 0.00047378582530654967, 0.003472051350399852, 0.002276513259857893, 0.0009407904581166804, 0.00122916663531214, 0.004373522009700537, 0.002106152707710862, 0.0048219277523458, 0.0015040780417621136, 0.0067988247610628605, 0.014766261912882328, 0.007154178339987993, 0.014550702646374702, 0.016012568026781082, 0.9022898077964783, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01750064641237259, 0.00032006113906390965, 0.0004851363191846758, 0.0008190704975277185, 0.0014002886600792408, 0.0002099828125210479, 0.00039746190304867923, 0.00033902807626873255, 0.0008702263585291803, 0.0017669523367658257, 0.0003638947382569313, 0.001575722824782133, 0.0003046909987460822, 0.0008767338586039841, 0.0011476021027192473, 0.0029251857195049524, 0.0015366642037406564, 0.007308902218937874, 0.002861541463062167, 0.009891731664538383, 0.014205148443579674, 0.008440139703452587, 0.041318219155073166, 0.8831350207328796, 0.0, 0.0, 0.0, 0.0], [0.022868743166327477, 0.004575476981699467, 0.004243134055286646, 0.0029073618352413177, 0.016250615939497948, 0.0037602137308567762, 0.006777574773877859, 0.005805710796266794, 0.001749276416376233, 0.007991507649421692, 0.007610088214278221, 0.002415310824289918, 0.017819568514823914, 0.0072409287095069885, 0.0030248560942709446, 0.029833368957042694, 0.012035910971462727, 0.02765420451760292, 0.047374121844768524, 0.018653549253940582, 0.05513159558176994, 0.03438228741288185, 0.15078744292259216, 0.25713324546813965, 0.2519738972187042, 0.0, 0.0, 0.0], [0.0031712385825812817, 0.000980205601081252, 0.0032804340589791536, 0.00041049442370422184, 0.0015932893147692084, 0.002033558441326022, 0.0011377325281500816, 0.0020856698974967003, 0.0005241815815679729, 0.000981826800853014, 0.001183110405690968, 0.001678483677096665, 0.0022669320460408926, 0.0026178329717367887, 0.0038217578548938036, 0.008246326819062233, 0.0027654231525957584, 0.028445959091186523, 0.016496427357196808, 0.008005425333976746, 0.0495024137198925, 0.029684454202651978, 0.024336421862244606, 0.1298293024301529, 0.06484737992286682, 0.6100736856460571, 0.0, 0.0], [0.07908482849597931, 0.0009674449102021754, 0.004016923252493143, 0.0023273269180208445, 0.006989642046391964, 0.0019026481313630939, 0.0005034904461354017, 0.001571505330502987, 0.0008593393722549081, 0.003415649989619851, 0.0018182016210630536, 0.001282359822653234, 0.003223744686692953, 0.003570053493604064, 0.0007532517774961889, 0.000774719868786633, 0.0008092733914963901, 0.0027114402037113905, 0.0029820066411048174, 0.0035971016623079777, 0.009571441449224949, 0.006415387149900198, 0.0257500521838665, 0.504065752029419, 0.007185800466686487, 0.02081139013171196, 0.30303919315338135, 0.0], [0.016588859260082245, 0.004048633389174938, 0.002222853945568204, 0.005555224604904652, 0.0033517007250338793, 0.000816008890978992, 0.0007398648886010051, 0.0007655283552594483, 0.0036786154378205538, 0.0030038775876164436, 0.00225289654918015, 0.0059513552114367485, 0.0019383537583053112, 0.0038933358155190945, 0.00788459088653326, 0.006405352149158716, 0.008325478062033653, 0.016746265813708305, 0.010980077087879181, 0.038261253386735916, 0.022972093895077705, 0.06862650066614151, 0.028807369992136955, 0.07900100201368332, 0.050660405308008194, 0.06107761338353157, 0.08716870844364166, 0.4582761228084564]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1932608187198639, 0.8067392110824585, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09132399410009384, 0.0022218949161469936, 0.9064540863037109, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.271509051322937, 0.05822678655385971, 0.04800604283809662, 0.6222581267356873, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04784078523516655, 0.00035570666659623384, 0.000596650002989918, 3.822638973360881e-05, 0.9511685967445374, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.019256873056292534, 0.0011810746509581804, 0.0001765929046086967, 5.894142304896377e-06, 0.000122228215332143, 0.9792574048042297, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.005920303985476494, 5.531004353542812e-05, 7.060235657263547e-05, 1.8034765162155963e-05, 3.028212086064741e-05, 0.00011463774717412889, 0.9937908053398132, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00194000534247607, 0.00011886676657013595, 8.414643525611609e-05, 2.4716589450690662e-06, 8.156303374562413e-05, 3.272992444181e-06, 0.0001161375839728862, 0.9976535439491272, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07166004925966263, 0.011905346997082233, 0.007623120676726103, 0.10244793444871902, 0.0006967833614908159, 0.0027506486512720585, 0.0017390804132446647, 0.0008579438435845077, 0.8003191947937012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01566774956882, 0.00026484750560484827, 3.115876825177111e-05, 2.6928866645903327e-05, 0.006772353779524565, 1.3349176697374787e-05, 6.307807052507997e-05, 3.1362655136035755e-05, 2.171032747355639e-06, 0.9771270155906677, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0020389079581946135, 7.648704922758043e-06, 2.724246951402165e-05, 8.47603860165691e-06, 6.465305887104478e-06, 1.3326608495844994e-05, 1.4772165286558447e-06, 3.915430625056615e-06, 1.5850508816583897e-07, 4.051489304401912e-05, 0.9978519678115845, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07929976284503937, 0.03803252428770065, 0.034687504172325134, 0.3190310597419739, 0.006834669969975948, 0.006867976859211922, 0.005305700935423374, 0.0073930430226027966, 0.07278455793857574, 0.002556875813752413, 0.006565558258444071, 0.4206407368183136, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.007406693417578936, 0.00014283796190284193, 0.00010935859609162435, 1.805660576792434e-05, 6.672463314316701e-06, 0.010674954392015934, 8.926730515668169e-05, 3.512840066832723e-06, 1.1229044503124896e-05, 6.144496751403494e-07, 6.924162153154612e-05, 2.826578793246881e-06, 0.9814648032188416, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.021147940307855606, 9.630298154661432e-05, 4.6663775719935074e-05, 0.00015684064419474453, 3.5300705349072814e-05, 0.000564930378459394, 0.0008296308224089444, 5.862339094164781e-05, 2.2225945940590464e-05, 2.856394712580368e-05, 9.341378608951345e-05, 7.3555206654418726e-06, 0.0003916253335773945, 0.9765204787254333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.14089550077915192, 0.1953195184469223, 0.0469839833676815, 0.12304626405239105, 0.034830380231142044, 0.013863453641533852, 0.011090556159615517, 0.019594747573137283, 0.06389324367046356, 0.021695252507925034, 0.02052069641649723, 0.05291970074176788, 0.005668587051331997, 0.010846426710486412, 0.2388317584991455, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0010231833439320326, 2.1616639060084708e-05, 1.7037547877407633e-05, 5.304920591697737e-07, 4.512396117206663e-06, 5.344834903553419e-07, 7.39133909632983e-08, 8.800788009466487e-07, 5.361462172004394e-07, 3.933455445803702e-06, 1.7785322370400536e-06, 1.6229247279397896e-08, 7.268410229244182e-08, 3.163024757668609e-07, 6.377179317951231e-08, 0.9989249110221863, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.034456267952919006, 0.2551186978816986, 0.016035139560699463, 0.00045389821752905846, 5.0874830776592717e-05, 0.0006998509052209556, 3.660436414065771e-05, 5.9108151617692783e-05, 9.472780220676214e-05, 3.513683986966498e-05, 8.401647392020095e-06, 0.00022237027587834746, 7.302349695237353e-05, 5.082943971501663e-05, 0.00022933543368708342, 0.00018693141464609653, 0.6921887397766113, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03124348074197769, 0.0020825532265007496, 0.0011515069054439664, 0.00017183022282551974, 0.00023733044508844614, 3.521403777995147e-05, 0.0002432404871797189, 0.0009225695976056159, 0.00018178475147578865, 2.062630301224999e-05, 0.00013406343350652605, 9.079599112737924e-05, 2.0748246242874302e-05, 2.0776817109435797e-05, 6.364433647831902e-05, 8.123941370286047e-05, 0.00017721908807288855, 0.963121235370636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0018971910467371345, 0.000224029048695229, 0.0004875531594734639, 3.942356215702603e-06, 2.763374322967138e-05, 3.1681032851338387e-05, 6.512104300782084e-05, 0.0014320060145109892, 7.957109460221545e-07, 1.9019996955194074e-07, 3.842447767965496e-06, 3.0876788059686078e-06, 8.914625504985452e-05, 2.3709066226729192e-05, 5.568791721088928e-07, 1.6088779375422746e-05, 3.361325434525497e-05, 3.5952880352851935e-06, 0.9956562519073486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03461822122335434, 0.066022127866745, 0.04419860243797302, 0.29248619079589844, 0.016893375664949417, 0.009481148794293404, 0.0038630825001746416, 0.008633027784526348, 0.048985555768013, 0.005254292860627174, 0.014528709463775158, 0.037307608872652054, 0.0033441614359617233, 0.0034130490384995937, 0.01775573380291462, 0.0025893826968967915, 0.012438114732503891, 0.024246862158179283, 0.0015039436984807253, 0.35243675112724304, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00030110403895378113, 0.00014472808106802404, 0.00025311458739452064, 5.297627012623707e-07, 6.429301720345393e-06, 9.658914495958015e-05, 0.00012203562073409557, 0.00011675567657221109, 1.0627870494772651e-07, 1.9750963531350862e-07, 9.606990261090687e-07, 9.529802014185407e-07, 8.967445864982437e-06, 4.1709995457495097e-07, 9.542875289980657e-08, 1.8979295646204264e-06, 6.7291812229086645e-06, 1.6589848428338883e-06, 0.001935696112923324, 2.2772167085349793e-07, 0.9970008730888367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.041367027908563614, 0.08478599041700363, 0.023927967995405197, 0.24218924343585968, 0.009730475954711437, 0.007324873004108667, 0.014812460169196129, 0.007613129913806915, 0.07007774710655212, 0.008909153752028942, 0.01480528898537159, 0.05801096558570862, 0.007966217584908009, 0.0072390493005514145, 0.02884133905172348, 0.004535495303571224, 0.010118664242327213, 0.047883063554763794, 0.0032959305681288242, 0.05003351718187332, 0.007403463590890169, 0.24912893772125244, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.003232732182368636, 1.1129427548439708e-05, 4.515576074481942e-05, 3.80717165171518e-06, 5.1710296247620136e-05, 7.245372671604855e-06, 0.0005140661960467696, 1.1736882697732653e-05, 6.655233164565288e-07, 4.771368548972532e-05, 0.0001914150925586, 2.697008767427178e-07, 3.651909082691418e-06, 7.421613645419711e-06, 1.3571271040291322e-07, 2.166434796890826e-06, 5.895946060263668e-07, 4.5659007810172625e-06, 4.359053036750993e-06, 1.1660046794759182e-07, 3.6440153508010553e-06, 6.861429113769191e-08, 0.9958556294441223, 0.0, 0.0, 0.0, 0.0, 0.0], [0.004403804894536734, 0.0003257049829699099, 0.00047201523557305336, 4.6670316805830225e-05, 0.0024762311950325966, 0.00028492463752627373, 0.00014090650074649602, 0.0004915344179607928, 1.0167791515414137e-06, 0.0003488078073132783, 0.00014969453332014382, 3.174719495291356e-06, 5.1670129323611036e-05, 2.2525975509779528e-05, 5.800658868793107e-07, 2.76809987553861e-06, 4.720895049104001e-06, 5.666921879310394e-06, 0.00011084787547588348, 1.4025960126673453e-06, 0.0001521978119853884, 1.8523221569921589e-06, 1.5057140444696415e-05, 0.9904863238334656, 0.0, 0.0, 0.0, 0.0], [0.021785754710435867, 0.06011905148625374, 0.022907676175236702, 0.12609848380088806, 0.02986062318086624, 0.004008305259048939, 0.01669515110552311, 0.003117478219792247, 0.00976229552179575, 0.018996462225914, 0.003068541642278433, 0.034128788858652115, 0.0011617466807365417, 0.001604503602720797, 0.02097986824810505, 0.001644492382183671, 0.013192826882004738, 0.002745883073657751, 0.0011626107152551413, 0.015526406466960907, 0.0009558351011946797, 0.011328496970236301, 0.0010903822258114815, 0.006449509412050247, 0.57160884141922, 0.0, 0.0, 0.0], [0.0038357703015208244, 0.0023991039488464594, 0.0011797187617048621, 0.0002615004777908325, 0.0002615117118693888, 0.000403466634452343, 0.0003234037139918655, 8.772678120294586e-05, 7.330311746045481e-06, 5.685877476935275e-05, 3.005909275088925e-05, 1.6612466424703598e-05, 1.8325770724914037e-05, 1.92932511708932e-05, 2.4646772089909064e-06, 2.3176198737928644e-05, 4.5322994992602617e-05, 1.5288241002053837e-06, 0.00011944754078285769, 6.977929388085613e-06, 0.0003188258269801736, 1.3718184163735714e-05, 8.017642358026933e-06, 2.828312608471606e-05, 1.8126912664229167e-06, 0.9905298352241516, 0.0, 0.0], [0.011135459877550602, 5.668294397764839e-05, 0.00019868848903570324, 4.231378989061341e-05, 1.234731826116331e-05, 0.00029469793662428856, 1.705147406028118e-05, 1.5092101648406242e-06, 1.761831845215056e-05, 3.643825948529411e-06, 8.908366726245731e-05, 2.464856606820831e-06, 0.0007188317831605673, 0.00014074365026317537, 4.491445224630297e-07, 1.3894714356865734e-05, 2.7840305847348645e-06, 3.4262063763890183e-06, 4.498209818848409e-05, 4.2242382392032596e-07, 5.275725470710313e-06, 2.2702904516336275e-06, 2.4455000584566733e-06, 1.6571835658396594e-05, 3.491482232220733e-07, 1.190479565593705e-06, 0.9871748089790344, 0.0], [0.015589498914778233, 0.0019515524618327618, 0.0009054269175976515, 0.0003336347290314734, 0.0021429008338600397, 0.00019534824241418391, 9.458114800509065e-05, 0.00011027580330846831, 8.39202111819759e-05, 0.0005385760450735688, 0.00014176475815474987, 9.287960710935295e-05, 4.8015484935604036e-05, 2.451350519550033e-05, 0.0001465167006244883, 0.01011804398149252, 0.0006290877354331315, 0.00027013852377422154, 0.0002486602170392871, 5.837721619172953e-05, 0.0001496377808507532, 9.529796079732478e-05, 1.1490618817333598e-05, 1.5781455658725463e-05, 0.00018300011288374662, 0.00014803851081524044, 9.807394235394895e-05, 0.9655750393867493]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9360750913619995, 0.06392484903335571, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8446658253669739, 0.06946855783462524, 0.08586560189723969, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6243894100189209, 0.14386312663555145, 0.20264333486557007, 0.029104186221957207, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5977473258972168, 0.04248015210032463, 0.09377884864807129, 0.05205114930868149, 0.21394257247447968, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5077127814292908, 0.03310268744826317, 0.0721021518111229, 0.03429172560572624, 0.14819985628128052, 0.2045908272266388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4338233470916748, 0.043457284569740295, 0.08453965187072754, 0.027778107672929764, 0.11795879155397415, 0.1466771811246872, 0.14576561748981476, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.37215620279312134, 0.02220938540995121, 0.03537566214799881, 0.042277391999959946, 0.10158123075962067, 0.13569514453411102, 0.16101622581481934, 0.12968876957893372, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.12500667572021484, 0.0278399009257555, 0.0934804379940033, 0.012690277770161629, 0.11638951301574707, 0.13336710631847382, 0.22109803557395935, 0.25271502137184143, 0.01741299405694008, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.19692790508270264, 0.029546214267611504, 0.04996200278401375, 0.020782943814992905, 0.11631420254707336, 0.10274212807416916, 0.1143372505903244, 0.215315043926239, 0.019354436546564102, 0.13471783697605133, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2227223664522171, 0.03360006958246231, 0.04074067994952202, 0.038002658635377884, 0.09921015053987503, 0.07817549258470535, 0.0640200674533844, 0.09184127300977707, 0.024441737681627274, 0.19122374057769775, 0.11602170765399933, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.08521247655153275, 0.017477311193943024, 0.03221548721194267, 0.007827759720385075, 0.09159605950117111, 0.11424333602190018, 0.10653657466173172, 0.13350805640220642, 0.010379147715866566, 0.15213122963905334, 0.24327799677848816, 0.005594557151198387, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1682017296552658, 0.01578889973461628, 0.0310809388756752, 0.012617113068699837, 0.0722142830491066, 0.1173223927617073, 0.16368074715137482, 0.15277527272701263, 0.01032545231282711, 0.07214081287384033, 0.07040023058652878, 0.008646306581795216, 0.10480575263500214, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2267676740884781, 0.01604059711098671, 0.024743622168898582, 0.00705508328974247, 0.04883765056729317, 0.10765626281499863, 0.13219675421714783, 0.10671056807041168, 0.008011599071323872, 0.06746657937765121, 0.07683229446411133, 0.0065303947776556015, 0.08443891257047653, 0.08671201020479202, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10061796009540558, 0.012468638829886913, 0.022032365202903748, 0.004160345997661352, 0.0734882801771164, 0.056621894240379333, 0.09245450794696808, 0.09395396709442139, 0.004900767467916012, 0.18768936395645142, 0.1626325249671936, 0.0024915027897804976, 0.10582542419433594, 0.07874027639627457, 0.0019222036935389042, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.14267098903656006, 0.021456196904182434, 0.02789515256881714, 0.021027902141213417, 0.06780445575714111, 0.045188818126916885, 0.07447187602519989, 0.12191041558980942, 0.019091788679361343, 0.09369932115077972, 0.08811882138252258, 0.02456088364124298, 0.0571986548602581, 0.06192152947187424, 0.024353735148906708, 0.1086294874548912, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.06704738736152649, 0.01030910387635231, 0.026336316019296646, 0.004695659037679434, 0.05926317721605301, 0.08496281504631042, 0.080722376704216, 0.11657446622848511, 0.003403414273634553, 0.11040167510509491, 0.14169591665267944, 0.0034816924016922712, 0.10041020810604095, 0.06070669740438461, 0.0030000011902302504, 0.12089251726865768, 0.006096551660448313, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.056178394705057144, 0.005979247856885195, 0.014519471675157547, 0.004990540444850922, 0.06327146291732788, 0.0510125532746315, 0.05109843611717224, 0.10485232621431351, 0.006269403733313084, 0.11892927438020706, 0.1270987093448639, 0.00434482516720891, 0.07219821959733963, 0.07194526493549347, 0.005287733860313892, 0.2191299945116043, 0.007506544701755047, 0.015387632884085178, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11726182699203491, 0.008147822692990303, 0.01787693053483963, 0.011206400580704212, 0.05595187097787857, 0.0836283341050148, 0.06390120834112167, 0.10258512943983078, 0.011110411025583744, 0.06166587769985199, 0.07090388238430023, 0.01114021334797144, 0.09706044942140579, 0.10543806850910187, 0.00975265633314848, 0.10426180064678192, 0.0074603371322155, 0.018806133419275284, 0.04184064641594887, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04574229195713997, 0.007668721489608288, 0.018333353102207184, 0.002340224338695407, 0.032237615436315536, 0.03437655419111252, 0.03381826728582382, 0.062048155814409256, 0.011586419306695461, 0.044029686599969864, 0.06360442191362381, 0.0041609336622059345, 0.030969934538006783, 0.03980116918683052, 0.003943989053368568, 0.1777312159538269, 0.00820691604167223, 0.036636702716350555, 0.07794366776943207, 0.264819860458374, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10117798298597336, 0.006271854043006897, 0.01573844440281391, 0.01417462807148695, 0.0792573019862175, 0.06329671293497086, 0.05208256095647812, 0.09634239971637726, 0.01153455302119255, 0.0918312594294548, 0.09554624557495117, 0.008432586677372456, 0.06626038253307343, 0.0576651468873024, 0.01867333985865116, 0.08391279727220535, 0.004163975827395916, 0.013387095183134079, 0.05073239281773567, 0.014733940362930298, 0.05478452518582344, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04546806216239929, 0.006257148459553719, 0.01680193655192852, 0.002446933416649699, 0.04770360514521599, 0.04247817024588585, 0.06154478341341019, 0.08222649991512299, 0.004518789239227772, 0.09918870031833649, 0.11719012260437012, 0.0019506969256326556, 0.06753381341695786, 0.06568485498428345, 0.002419339492917061, 0.1423516869544983, 0.006389489397406578, 0.025134507566690445, 0.06385159492492676, 0.007930083200335503, 0.08700327575206757, 0.003925919067114592, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10133185982704163, 0.008225103840231895, 0.018161959946155548, 0.011032741516828537, 0.055563781410455704, 0.06697413325309753, 0.04799129068851471, 0.05184986814856529, 0.010775167495012283, 0.04530857503414154, 0.052137237042188644, 0.013668352738022804, 0.11421096324920654, 0.13324011862277985, 0.011244880966842175, 0.05115794390439987, 0.010221034288406372, 0.012498279102146626, 0.03905014321208, 0.0073533933609724045, 0.03256060928106308, 0.0210000891238451, 0.08444252610206604, 0.0, 0.0, 0.0, 0.0, 0.0], [0.13115419447422028, 0.006403299514204264, 0.016545388847589493, 0.005188062787055969, 0.07789057493209839, 0.08286862820386887, 0.057068560272455215, 0.09789616614580154, 0.0043412744998931885, 0.05733862146735191, 0.06461106985807419, 0.003735023085027933, 0.07044798880815506, 0.043697290122509, 0.006026663817465305, 0.05570601671934128, 0.004359432961791754, 0.009181264787912369, 0.04220930114388466, 0.0025162827223539352, 0.0405394546687603, 0.005964982323348522, 0.056577179580926895, 0.05773324891924858, 0.0, 0.0, 0.0, 0.0], [0.057750146836042404, 0.0087440712377429, 0.025541240349411964, 0.00197264994494617, 0.028591617941856384, 0.020516544580459595, 0.034858766943216324, 0.05235983803868294, 0.002563845831900835, 0.09297135472297668, 0.09860754013061523, 0.0023141643032431602, 0.0801919475197792, 0.07177726179361343, 0.002441423013806343, 0.09027043730020523, 0.006848265882581472, 0.012407878413796425, 0.052052922546863556, 0.006691231857985258, 0.05122021585702896, 0.003103853203356266, 0.1459810435771942, 0.04802256077528, 0.002199268201366067, 0.0, 0.0, 0.0], [0.04853994771838188, 0.00926976278424263, 0.023928552865982056, 0.007731621619313955, 0.04801534488797188, 0.026766527444124222, 0.02424401044845581, 0.04310622438788414, 0.012635767459869385, 0.08389179408550262, 0.057351287454366684, 0.008199063129723072, 0.04182906076312065, 0.026869865134358406, 0.019102562218904495, 0.08853888511657715, 0.010595502331852913, 0.029346970841288567, 0.055591873824596405, 0.010479518212378025, 0.0658007338643074, 0.011931170709431171, 0.12403426319360733, 0.07292608916759491, 0.010737767443060875, 0.038535766303539276, 0.0, 0.0], [0.1263422966003418, 0.0031891444232314825, 0.008747638203203678, 0.003811490023508668, 0.05773730203509331, 0.052271146327257156, 0.08296675980091095, 0.09625151008367538, 0.0030196113511919975, 0.051326580345630646, 0.048148639500141144, 0.0019345232285559177, 0.06429490447044373, 0.07124300301074982, 0.004750909749418497, 0.029667386785149574, 0.0018589231185615063, 0.002846652874723077, 0.04352571815252304, 0.0019522737711668015, 0.023465242236852646, 0.0032035368494689465, 0.05552155524492264, 0.07096351683139801, 0.003476083278656006, 0.005641535390168428, 0.08184212446212769, 0.0], [0.07321730256080627, 0.006267612334340811, 0.01156355906277895, 0.004792156629264355, 0.042489632964134216, 0.04371749982237816, 0.05060166120529175, 0.03159410133957863, 0.00446929270401597, 0.08724582940340042, 0.05620451644062996, 0.005324976518750191, 0.06135175749659538, 0.029356511309742928, 0.0045090047642588615, 0.04587344825267792, 0.0043631731532514095, 0.009604141116142273, 0.019971752539277077, 0.00544841168448329, 0.03208650276064873, 0.006401734426617622, 0.12427368760108948, 0.052210722118616104, 0.011708194389939308, 0.022411203011870384, 0.1367776095867157, 0.016164034605026245]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9780336022377014, 0.021966326981782913, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5068986415863037, 0.42791470885276794, 0.06518669426441193, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.35398754477500916, 0.20986193418502808, 0.22097158432006836, 0.2151789516210556, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2843107283115387, 0.09566715359687805, 0.10258682072162628, 0.27493274211883545, 0.24250255525112152, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.15383920073509216, 0.047544628381729126, 0.08730429410934448, 0.13800546526908875, 0.43880021572113037, 0.1345062404870987, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10676965862512589, 0.028421832248568535, 0.01951194927096367, 0.09416323155164719, 0.20806139707565308, 0.402135968208313, 0.14093591272830963, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11149723082780838, 0.04616847261786461, 0.04144929721951485, 0.09405724704265594, 0.12141997367143631, 0.26662734150886536, 0.2864832282066345, 0.03229719400405884, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.08988063782453537, 0.027017561718821526, 0.02024621143937111, 0.047715287655591965, 0.09914563596248627, 0.09427515417337418, 0.12657426297664642, 0.2254263162612915, 0.2697189152240753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0998041033744812, 0.022541336715221405, 0.015702545642852783, 0.04647798836231232, 0.05816418305039406, 0.021747710183262825, 0.055433548986911774, 0.08357411623001099, 0.3198898136615753, 0.27666470408439636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07597794383764267, 0.014713923446834087, 0.03084854781627655, 0.027346162125468254, 0.05854067578911781, 0.009656218811869621, 0.019951749593019485, 0.022526316344738007, 0.15989556908607483, 0.49656784534454346, 0.08397496491670609, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04108642414212227, 0.010004311800003052, 0.005554011557251215, 0.0163047444075346, 0.020537275820970535, 0.029098205268383026, 0.04194561392068863, 0.06296415627002716, 0.08651114255189896, 0.22713392972946167, 0.24420025944709778, 0.2146598994731903, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05004925653338432, 0.013216468505561352, 0.003358508925884962, 0.017261970788240433, 0.07628092914819717, 0.021910332143306732, 0.029213745146989822, 0.03811021149158478, 0.06309700012207031, 0.15392108261585236, 0.1122962236404419, 0.2423628270626068, 0.17892150580883026, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05410429462790489, 0.011767620220780373, 0.0040957992896437645, 0.011731634847819805, 0.05059590935707092, 0.007858783937990665, 0.011649033986032009, 0.0247200857847929, 0.06056256592273712, 0.12994155287742615, 0.1281975358724594, 0.19594936072826385, 0.15464812517166138, 0.15417766571044922, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.029019562527537346, 0.006497912108898163, 0.005325503647327423, 0.0064343842677772045, 0.004586254246532917, 0.0046164495870471, 0.007444112561643124, 0.008928060531616211, 0.024712448939681053, 0.04043995961546898, 0.07518034428358078, 0.08614151924848557, 0.059629011899232864, 0.18675260245800018, 0.4542919397354126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03134073317050934, 0.010292730294167995, 0.0061365640722215176, 0.008080936968326569, 0.011057039722800255, 0.006439926102757454, 0.0037622086238116026, 0.012260666117072105, 0.023032650351524353, 0.01401376910507679, 0.022073302417993546, 0.06507188081741333, 0.08302082121372223, 0.017168767750263214, 0.6193152070045471, 0.06693282723426819, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.020297516137361526, 0.001680320012383163, 0.0010053697042167187, 0.003559119300916791, 0.00342841655947268, 0.0015060395235195756, 0.005790371913462877, 0.004576757084578276, 0.009393969550728798, 0.013879864476621151, 0.016031647101044655, 0.03081718273460865, 0.01571531593799591, 0.0809771865606308, 0.3039557635784149, 0.28339967131614685, 0.20398549735546112, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.015808476135134697, 0.0019271880155429244, 0.0012134852586314082, 0.002690856112167239, 0.0024934245739132166, 0.001745802816003561, 0.0021940397564321756, 0.0048294588923454285, 0.005832239519804716, 0.008535100147128105, 0.011157487519085407, 0.01793346181511879, 0.026477662846446037, 0.053386617451906204, 0.19046907126903534, 0.13742172718048096, 0.306815505027771, 0.20906835794448853, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.020579341799020767, 0.0034562288783490658, 0.0013491312274709344, 0.003367807948961854, 0.0033614833373576403, 0.006162772886455059, 0.005714510567486286, 0.0018352374900132418, 0.006571010686457157, 0.005628711078315973, 0.009594970382750034, 0.01899225451052189, 0.017214586958289146, 0.03198767453432083, 0.1287742555141449, 0.10816487669944763, 0.21935690939426422, 0.3339385390281677, 0.07394972443580627, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01624627225100994, 0.0016583274118602276, 0.0013173538027331233, 0.0016637893859297037, 0.002212703227996826, 0.0014318311586976051, 0.0018618459580466151, 0.0021754379849880934, 0.002462927717715502, 0.005211576819419861, 0.005885979626327753, 0.006869680248200893, 0.013401443138718605, 0.022392215207219124, 0.08261127769947052, 0.10992788523435593, 0.12928259372711182, 0.1350434273481369, 0.21382689476013184, 0.2445165514945984, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01220993883907795, 0.002239619381725788, 0.0012634454760700464, 0.0018843056168407202, 0.0024663882795721292, 0.00232436484657228, 0.0007448937394656241, 0.0014206132618710399, 0.0027054043021053076, 0.003143860725685954, 0.00855665747076273, 0.006706365384161472, 0.004479723982512951, 0.015570293180644512, 0.06112739443778992, 0.022037800401449203, 0.14897388219833374, 0.11252952367067337, 0.1505308449268341, 0.3198215663433075, 0.11926315724849701, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.016678182408213615, 0.002112425398081541, 0.0014118484687060118, 0.0018443010048940778, 0.0017354742158204317, 0.0019877906888723373, 0.0016780367586761713, 0.0022542541846632957, 0.0019330320646986365, 0.0033869210164994, 0.005212709773331881, 0.0044935960322618484, 0.007686940021812916, 0.011426188983023167, 0.04000218212604523, 0.038981422781944275, 0.07752230018377304, 0.06171416491270065, 0.07038114964962006, 0.12241952121257782, 0.14507083594799042, 0.38006672263145447, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.018559450283646584, 0.0015224030939862132, 0.0009748733136802912, 0.0022228413727134466, 0.001326504978351295, 0.0023066196590662003, 0.0015994164859876037, 0.0005631495150737464, 0.0018668514676392078, 0.0035763359628617764, 0.00427754083648324, 0.004995701368898153, 0.006241795606911182, 0.012372647412121296, 0.028770819306373596, 0.01661060005426407, 0.037285394966602325, 0.06706205010414124, 0.051107387989759445, 0.13387271761894226, 0.08051225543022156, 0.3328489363193512, 0.18952371180057526, 0.0, 0.0, 0.0, 0.0, 0.0], [0.029027987271547318, 0.0015451067592948675, 0.0017601873259991407, 0.002711227163672447, 0.004479327239096165, 0.0011998101836070418, 0.003328657476231456, 0.0017611562507227063, 0.002338737016543746, 0.0023528586607426405, 0.0036765309050679207, 0.004161324352025986, 0.005122391041368246, 0.003214112250134349, 0.026016563177108765, 0.01696649193763733, 0.02585640549659729, 0.03612591326236725, 0.016752729192376137, 0.08183567225933075, 0.07990510016679764, 0.2683999836444855, 0.2495778650045395, 0.13188384473323822, 0.0, 0.0, 0.0, 0.0], [0.013491827994585037, 0.0013995375484228134, 0.0014146931935101748, 0.0012795566581189632, 0.0025481677148491144, 0.001222207909449935, 0.0012458552373573184, 0.00135324546135962, 0.0009537729783914983, 0.0022019895259290934, 0.0018299530493095517, 0.0016625769203528762, 0.005244024563580751, 0.008667188696563244, 0.013842331245541573, 0.02967905066907406, 0.023402370512485504, 0.02465790882706642, 0.041152168065309525, 0.035446587949991226, 0.04745902121067047, 0.13833536207675934, 0.1650981605052948, 0.21336618065834045, 0.22304630279541016, 0.0, 0.0, 0.0], [0.015361586585640907, 0.0033277045004069805, 0.00159774674102664, 0.0016916352324187756, 0.0006014645332470536, 0.0017118450487032533, 0.0006461005541495979, 0.0006020137225277722, 0.0009884729515761137, 0.0008836572524160147, 0.0011778350453823805, 0.0023478101938962936, 0.0006413549999706447, 0.00082437350647524, 0.016818776726722717, 0.01854679547250271, 0.018054869025945663, 0.02313813753426075, 0.013480539433658123, 0.043264105916023254, 0.07498486340045929, 0.12014006823301315, 0.08853135257959366, 0.1757366955280304, 0.2582874596118927, 0.1166127622127533, 0.0, 0.0], [0.025348607450723648, 0.00172924913931638, 0.003286223392933607, 0.0030570419039577246, 0.0026735796127468348, 0.00422022957354784, 0.0010710919741541147, 0.0019506510579958558, 0.0011808705748990178, 0.0014609310310333967, 0.0021281877998262644, 0.0016712831566110253, 0.00762981828302145, 0.00455628102645278, 0.016527287662029266, 0.004682433791458607, 0.01230490393936634, 0.009135307744145393, 0.019666370004415512, 0.025566307827830315, 0.05532284080982208, 0.08650508522987366, 0.18945632874965668, 0.15576964616775513, 0.13003437221050262, 0.17274869978427887, 0.060316383838653564, 0.0], [0.012738823890686035, 0.002302758628502488, 0.0018821185221895576, 0.0018223758088424802, 0.0006622060900554061, 0.00014638279390055686, 0.0004558951186481863, 0.00037728340248577297, 0.0005465536960400641, 0.00044157932279631495, 0.0008529919432476163, 0.0010844074422493577, 0.0008667440270073712, 0.0004505232209339738, 0.004670339170843363, 0.013433186337351799, 0.007601807825267315, 0.009516512043774128, 0.005935702472925186, 0.014075146056711674, 0.01185080036520958, 0.04637249931693077, 0.026819732040166855, 0.03430616110563278, 0.11441628634929657, 0.13754068315029144, 0.129195898771286, 0.4196345806121826]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8970723748207092, 0.10292758792638779, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7731761336326599, 0.17108604311943054, 0.05573779717087746, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1262785941362381, 0.05244283378124237, 0.08141697198152542, 0.7398616075515747, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4982219636440277, 0.11716156452894211, 0.10303275287151337, 0.19211649894714355, 0.08946727216243744, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.47527164220809937, 0.1459793746471405, 0.06215833127498627, 0.1700431853532791, 0.09091276675462723, 0.055634740740060806, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3601544499397278, 0.07191380858421326, 0.13872478902339935, 0.20251165330410004, 0.09174146503210068, 0.08474547415971756, 0.05020834505558014, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.44650161266326904, 0.09384463727474213, 0.0769660621881485, 0.1308562159538269, 0.06314979493618011, 0.10885287076234818, 0.05553644895553589, 0.024292338639497757, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0853608027100563, 0.048039764165878296, 0.06279397010803223, 0.36496537923812866, 0.06284161657094955, 0.03294922411441803, 0.050738219171762466, 0.04514218494296074, 0.2471688985824585, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.22629579901695251, 0.05903681740164757, 0.052295126020908356, 0.12980739772319794, 0.10807495564222336, 0.05635429173707962, 0.06454531103372574, 0.07120639085769653, 0.11933315545320511, 0.11305069178342819, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2628517746925354, 0.05615796893835068, 0.044778011739254, 0.11963243037462234, 0.04854040592908859, 0.019181866198778152, 0.15023164451122284, 0.12276432663202286, 0.08969192951917648, 0.048059966415166855, 0.03810972347855568, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.058130137622356415, 0.020128706470131874, 0.040194325149059296, 0.28802230954170227, 0.026615263894200325, 0.0242946594953537, 0.028632590547204018, 0.02997012622654438, 0.16046138107776642, 0.07546462118625641, 0.04394363984465599, 0.204142227768898, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.27383044362068176, 0.07681199908256531, 0.015544225461781025, 0.059055645018815994, 0.021316805854439735, 0.03514791280031204, 0.20391517877578735, 0.08499471843242645, 0.0654987022280693, 0.06311772018671036, 0.018887363374233246, 0.04818867892026901, 0.03369062766432762, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2112003117799759, 0.05768706277012825, 0.02612086571753025, 0.06499532610177994, 0.05020938813686371, 0.03286217898130417, 0.05481945350766182, 0.03538234904408455, 0.07620051503181458, 0.10328488796949387, 0.03540724143385887, 0.07928034663200378, 0.1019224002957344, 0.07062768191099167, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07516643404960632, 0.016909845173358917, 0.019499367102980614, 0.09737149626016617, 0.008110840804874897, 0.007022075355052948, 0.008254083804786205, 0.008634628728032112, 0.06944439560174942, 0.011870723217725754, 0.01185668259859085, 0.09617085009813309, 0.011752291582524776, 0.016422292217612267, 0.5415140986442566, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.21299727261066437, 0.05554702877998352, 0.03354492038488388, 0.06980074197053909, 0.01192587148398161, 0.042620304971933365, 0.042740970849990845, 0.023551827296614647, 0.06118902936577797, 0.0334026999771595, 0.04747573658823967, 0.10559387505054474, 0.05825109779834747, 0.08898123353719711, 0.07287431508302689, 0.03950304910540581, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.08579542487859726, 0.022007210180163383, 0.035275883972644806, 0.1369931697845459, 0.022599197924137115, 0.032040901482105255, 0.02694125659763813, 0.031156841665506363, 0.08478567004203796, 0.04883565381169319, 0.02075711265206337, 0.09827975183725357, 0.016494708135724068, 0.035530731081962585, 0.1980716586112976, 0.03683561459183693, 0.06759919971227646, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05036327987909317, 0.06337293237447739, 0.02182626537978649, 0.05596266686916351, 0.014323081821203232, 0.022574009373784065, 0.011428963392972946, 0.0314309298992157, 0.05424933880567551, 0.027940331026911736, 0.012594304978847504, 0.04719426482915878, 0.017571642994880676, 0.01141622755676508, 0.1076088473200798, 0.0170487891882658, 0.28775763511657715, 0.1453365832567215, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.12433753907680511, 0.03811335191130638, 0.025785451754927635, 0.03346598520874977, 0.030228909105062485, 0.10008382797241211, 0.025684935972094536, 0.013476326130330563, 0.052352175116539, 0.0502220094203949, 0.04786545783281326, 0.06479834765195847, 0.08944874256849289, 0.028657497838139534, 0.04761061444878578, 0.026688024401664734, 0.08425813913345337, 0.10349573940038681, 0.013426993042230606, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.020184580236673355, 0.007319833151996136, 0.012613321654498577, 0.08851537853479385, 0.006193249952048063, 0.011044938117265701, 0.008082342334091663, 0.0076659685000777245, 0.06908395886421204, 0.019107472151517868, 0.015595674514770508, 0.08421090245246887, 0.009563443250954151, 0.0166443083435297, 0.17358432710170746, 0.010202936828136444, 0.03501289337873459, 0.106765016913414, 0.005935480818152428, 0.29267391562461853, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09175419807434082, 0.0662536770105362, 0.017244936898350716, 0.026713989675045013, 0.025204217061400414, 0.03129229322075844, 0.01644541323184967, 0.06892763823270798, 0.0478653609752655, 0.04581440985202789, 0.017736565321683884, 0.04659421369433403, 0.03348995000123978, 0.02422589808702469, 0.05400930717587471, 0.028572773560881615, 0.1406010538339615, 0.08887138962745667, 0.03131389990448952, 0.07584948092699051, 0.021219318732619286, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01740921474993229, 0.007177290972322226, 0.01293893437832594, 0.08538280427455902, 0.00625436520203948, 0.009704620577394962, 0.006120662670582533, 0.005790448747575283, 0.04115397483110428, 0.016947856172919273, 0.009982768446207047, 0.05812996253371239, 0.010317761451005936, 0.018283678218722343, 0.17058639228343964, 0.010652310214936733, 0.03902449831366539, 0.0768742486834526, 0.006444136146456003, 0.1412632018327713, 0.012537826783955097, 0.23702313005924225, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11050025373697281, 0.040565844625234604, 0.022842274978756905, 0.021627245470881462, 0.050500646233558655, 0.010971728712320328, 0.02190316841006279, 0.018156031146645546, 0.045538973063230515, 0.041428081691265106, 0.02441258542239666, 0.05504799261689186, 0.018197815865278244, 0.01793895848095417, 0.03656463325023651, 0.03427047282457352, 0.08616706728935242, 0.07797155529260635, 0.011862932704389095, 0.08286207914352417, 0.027029210701584816, 0.10406547039747238, 0.03957498446106911, 0.0, 0.0, 0.0, 0.0, 0.0], [0.06304915249347687, 0.02035381831228733, 0.021248817443847656, 0.04505620896816254, 0.017268503084778786, 0.05014343559741974, 0.01748841628432274, 0.02344980090856552, 0.04411298781633377, 0.039050597697496414, 0.033829934895038605, 0.042456913739442825, 0.05451410636305809, 0.07052335143089294, 0.043921053409576416, 0.03414224460721016, 0.03310287371277809, 0.03600842505693436, 0.018191633746027946, 0.06144999340176582, 0.028453592211008072, 0.09732427448034286, 0.049378957599401474, 0.05548081547021866, 0.0, 0.0, 0.0, 0.0], [0.0349343977868557, 0.00887538306415081, 0.012652167119085789, 0.07944805175065994, 0.011851700022816658, 0.0035884403623640537, 0.022012103348970413, 0.015129690989851952, 0.03899296745657921, 0.023887259885668755, 0.008657827973365784, 0.045190539211034775, 0.006684374529868364, 0.01799309253692627, 0.12867264449596405, 0.01209914218634367, 0.02328244037926197, 0.05521561950445175, 0.01002154778689146, 0.10659800469875336, 0.0078909145668149, 0.13145135343074799, 0.04886123538017273, 0.03015991859138012, 0.11584915965795517, 0.0, 0.0, 0.0], [0.03381861746311188, 0.01329686027020216, 0.021154114976525307, 0.032462432980537415, 0.015269575640559196, 0.011270202696323395, 0.015662292018532753, 0.0067288498394191265, 0.039436206221580505, 0.037036821246147156, 0.022056730464100838, 0.030956219881772995, 0.03584176301956177, 0.030775275081396103, 0.05931456387042999, 0.03594156727194786, 0.03348274528980255, 0.10148895531892776, 0.017201201990246773, 0.08388356119394302, 0.023550406098365784, 0.08805210143327713, 0.09201395511627197, 0.04169658571481705, 0.05884915590286255, 0.018759267404675484, 0.0, 0.0], [0.1006065309047699, 0.04403234273195267, 0.0165207888931036, 0.02349918894469738, 0.03232693672180176, 0.016552219167351723, 0.018802238628268242, 0.028956132009625435, 0.02990656904876232, 0.044001635164022446, 0.036051955074071884, 0.024858785793185234, 0.02582434006035328, 0.06607220321893692, 0.023305874317884445, 0.04206628352403641, 0.05689911171793938, 0.03745561093091965, 0.022992653772234917, 0.03209444507956505, 0.01904374547302723, 0.04857221245765686, 0.06155959889292717, 0.033440109342336655, 0.032227419316768646, 0.019226012751460075, 0.06310505419969559, 0.0], [0.06983619928359985, 0.012005629017949104, 0.006682232022285461, 0.026869941502809525, 0.0042119938880205154, 0.011844510212540627, 0.005950474180281162, 0.0032855025492608547, 0.03628860041499138, 0.010307921096682549, 0.012058786116540432, 0.05053987354040146, 0.01135439332574606, 0.02338586561381817, 0.11302169412374496, 0.010950861498713493, 0.0252560805529356, 0.03947985917329788, 0.004697861615568399, 0.09879378974437714, 0.009753737598657608, 0.1500122845172882, 0.010377990081906319, 0.011080796830356121, 0.14232048392295837, 0.010290198028087616, 0.032311566174030304, 0.05703091248869896]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8803902268409729, 0.1196097582578659, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8001625537872314, 0.14422571659088135, 0.055611707270145416, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.643402099609375, 0.14134983718395233, 0.0702095627784729, 0.14503847062587738, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6266937255859375, 0.1131797805428505, 0.08151954412460327, 0.12932473421096802, 0.04928220808506012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5475613474845886, 0.12484730780124664, 0.09718161076307297, 0.10719481110572815, 0.10159611701965332, 0.021618740633130074, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5087329149246216, 0.09915056824684143, 0.06520584970712662, 0.11256356537342072, 0.06782650947570801, 0.12408695369958878, 0.022433657199144363, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.48695340752601624, 0.09624222666025162, 0.0722932443022728, 0.10468455404043198, 0.08470048755407333, 0.0729580819606781, 0.06276600062847137, 0.01940201036632061, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3876568675041199, 0.09759235382080078, 0.06273507326841354, 0.10516118258237839, 0.06681734323501587, 0.052168652415275574, 0.045836515724658966, 0.06663621962070465, 0.1153956949710846, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3888711631298065, 0.07671713829040527, 0.0563858225941658, 0.09019602835178375, 0.05713658034801483, 0.07272129505872726, 0.05961492285132408, 0.05968064069747925, 0.08826401829719543, 0.0504123754799366, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.34817835688591003, 0.083405040204525, 0.059208229184150696, 0.06896515190601349, 0.07435943931341171, 0.06784242391586304, 0.0611436553299427, 0.06327974051237106, 0.07901298999786377, 0.06223493069410324, 0.03237001225352287, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3058014214038849, 0.07651673257350922, 0.04861818253993988, 0.08338676393032074, 0.05422047898173332, 0.04352279752492905, 0.04213728383183479, 0.0453234426677227, 0.0894325003027916, 0.0622859001159668, 0.05299954116344452, 0.09575489163398743, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.31689736247062683, 0.06513331830501556, 0.057403840124607086, 0.05402977764606476, 0.05759810283780098, 0.050990890711545944, 0.05446450784802437, 0.06690814346075058, 0.06280667334794998, 0.06919551640748978, 0.05741636082530022, 0.06416007876396179, 0.02299542911350727, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3205448091030121, 0.060393836349248886, 0.049071453511714935, 0.06111529469490051, 0.054544154554605484, 0.06291463971138, 0.03561454266309738, 0.048615507781505585, 0.062169838696718216, 0.05161335691809654, 0.06446864455938339, 0.06270692497491837, 0.04868292436003685, 0.017544066533446312, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2667199373245239, 0.07539885491132736, 0.045474860817193985, 0.059659991413354874, 0.04854734241962433, 0.03549426421523094, 0.03146343305706978, 0.047872792929410934, 0.06306471675634384, 0.05125550925731659, 0.044633544981479645, 0.06396524608135223, 0.03940986469388008, 0.04443803057074547, 0.08260153979063034, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3193580210208893, 0.05959266796708107, 0.040557608008384705, 0.04961322993040085, 0.03754067048430443, 0.03688221424818039, 0.037176571786403656, 0.051398150622844696, 0.048158664256334305, 0.034208521246910095, 0.04117461293935776, 0.05956944078207016, 0.03676413372159004, 0.04247494786977768, 0.0761006623506546, 0.0294298455119133, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2312609702348709, 0.05075312405824661, 0.030192501842975616, 0.052217960357666016, 0.038446661084890366, 0.03602057695388794, 0.032999712973833084, 0.039067342877388, 0.05245039984583855, 0.03905348479747772, 0.03879252448678017, 0.06319022923707962, 0.04473385214805603, 0.03920363262295723, 0.07573653012514114, 0.08049734681844711, 0.055383045226335526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.20345553755760193, 0.05053539201617241, 0.030341127887368202, 0.04316543787717819, 0.035112496465444565, 0.02917129546403885, 0.03183463588356972, 0.04399237781763077, 0.04888094216585159, 0.035785507410764694, 0.0383659303188324, 0.05589165911078453, 0.03678322583436966, 0.046046383678913116, 0.07057204097509384, 0.06881317496299744, 0.06799708306789398, 0.06325571984052658, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.21681667864322662, 0.04746149480342865, 0.03519782796502113, 0.039162926375865936, 0.035859428346157074, 0.03182743862271309, 0.030038507655262947, 0.03711697459220886, 0.03647560253739357, 0.03631875291466713, 0.041185490787029266, 0.04578869789838791, 0.04079161584377289, 0.04306900501251221, 0.06215693801641464, 0.06847190111875534, 0.05959145352244377, 0.06440314650535583, 0.02826613374054432, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.17228755354881287, 0.04134087264537811, 0.02879972942173481, 0.04868966341018677, 0.033580586314201355, 0.027123890817165375, 0.02261853963136673, 0.03440002351999283, 0.05095868557691574, 0.040706176310777664, 0.03536376729607582, 0.054970841854810715, 0.03044343926012516, 0.038766276091337204, 0.05966315418481827, 0.0560557059943676, 0.057048700749874115, 0.05457262694835663, 0.033928968012332916, 0.07868072390556335, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1673813760280609, 0.039784010499715805, 0.031261153519153595, 0.03522958979010582, 0.035942740738391876, 0.0257559921592474, 0.030036577954888344, 0.04101672023534775, 0.035010840743780136, 0.031883157789707184, 0.041192468255758286, 0.038738228380680084, 0.03981960564851761, 0.03998809680342674, 0.0583023801445961, 0.060018278658390045, 0.053186509758234024, 0.058574240654706955, 0.039404142647981644, 0.05486196279525757, 0.0426119863986969, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.16614678502082825, 0.039972975850105286, 0.027515441179275513, 0.03916836529970169, 0.03159745782613754, 0.02316010184586048, 0.02109248749911785, 0.027392204850912094, 0.041257716715335846, 0.031881183385849, 0.02800074592232704, 0.046095628291368484, 0.02929016202688217, 0.02912607043981552, 0.05312474071979523, 0.05457811430096626, 0.05156220123171806, 0.04956112056970596, 0.030013637617230415, 0.06773704290390015, 0.042935680598020554, 0.06879008561372757, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.15281905233860016, 0.02822752296924591, 0.027743125334382057, 0.028756925836205482, 0.03501783311367035, 0.05847910791635513, 0.026856685057282448, 0.03958284482359886, 0.028714003041386604, 0.03471685200929642, 0.032830651849508286, 0.03651346266269684, 0.04963479936122894, 0.0458652600646019, 0.045760300010442734, 0.06188223883509636, 0.03529275581240654, 0.03762689605355263, 0.03984266147017479, 0.04643535614013672, 0.04801805317401886, 0.04953726381063461, 0.009846359491348267, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2115374505519867, 0.0333629809319973, 0.032988984137773514, 0.04084452986717224, 0.03781212493777275, 0.031148619949817657, 0.019334593787789345, 0.026969699189066887, 0.03122800402343273, 0.03933965414762497, 0.022205635905265808, 0.0360027551651001, 0.03218095749616623, 0.02714257687330246, 0.04190076142549515, 0.06052219495177269, 0.03642525151371956, 0.034138087183237076, 0.031727783381938934, 0.048152077943086624, 0.03429953381419182, 0.05297607555985451, 0.026413550600409508, 0.01134606171399355, 0.0, 0.0, 0.0, 0.0], [0.15546412765979767, 0.03439837321639061, 0.023608434945344925, 0.03389633446931839, 0.029168589040637016, 0.022091854363679886, 0.023291269317269325, 0.023001449182629585, 0.034650519490242004, 0.03135782107710838, 0.025449620559811592, 0.037712085992097855, 0.02777543105185032, 0.026475755497813225, 0.0468711294233799, 0.05272452160716057, 0.03702209144830704, 0.036683451384305954, 0.026065684854984283, 0.05377717316150665, 0.032180055975914, 0.060315992683172226, 0.03606291487812996, 0.03783933073282242, 0.0521160252392292, 0.0, 0.0, 0.0], [0.13623987138271332, 0.03111819550395012, 0.02507743053138256, 0.03278560936450958, 0.030633894726634026, 0.026525607332587242, 0.022824661806225777, 0.021268373355269432, 0.030445408076047897, 0.030394796282052994, 0.027277056127786636, 0.0340714156627655, 0.036153558641672134, 0.025476135313510895, 0.04680701345205307, 0.041275057941675186, 0.03687120974063873, 0.036554452031850815, 0.03356430307030678, 0.05303466320037842, 0.03805123269557953, 0.05168144032359123, 0.045366570353507996, 0.028985250741243362, 0.04936319217085838, 0.028153587132692337, 0.0, 0.0], [0.1621684432029724, 0.028824398294091225, 0.026198962703347206, 0.0260844137519598, 0.03390086442232132, 0.03255961462855339, 0.026250438764691353, 0.03453149273991585, 0.028503205627202988, 0.029973438009619713, 0.026024989783763885, 0.02873058058321476, 0.02477167919278145, 0.02596430480480194, 0.03890504315495491, 0.05019637942314148, 0.03284754604101181, 0.03689656779170036, 0.03063713200390339, 0.03784448653459549, 0.041831858456134796, 0.041579943150281906, 0.036295901983976364, 0.04164512827992439, 0.031040694564580917, 0.027581991627812386, 0.018210506066679955, 0.0], [0.14884023368358612, 0.03391680866479874, 0.018652411177754402, 0.02600737102329731, 0.022966569289565086, 0.017573988065123558, 0.024054130539298058, 0.02180575206875801, 0.026083413511514664, 0.02296486496925354, 0.024161221459507942, 0.030358988791704178, 0.023586455732584, 0.02933933027088642, 0.03906520828604698, 0.04662427678704262, 0.03746568039059639, 0.03643304109573364, 0.022136172279715538, 0.03995424136519432, 0.03786343336105347, 0.04710649326443672, 0.030810659751296043, 0.040916558355093, 0.03838794678449631, 0.035201169550418854, 0.03676789999008179, 0.04095566272735596]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6533873081207275, 0.3466126620769501, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4451116919517517, 0.3337682783603668, 0.22111999988555908, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5567563772201538, 0.15201503038406372, 0.07401557266712189, 0.21721303462982178, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.46572428941726685, 0.10320363938808441, 0.06591737270355225, 0.11550311744213104, 0.24965165555477142, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3084735572338104, 0.16819123923778534, 0.10153278708457947, 0.10121862590312958, 0.07406841218471527, 0.24651534855365753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2615838944911957, 0.11091180145740509, 0.06571843475103378, 0.12967447936534882, 0.052850671112537384, 0.05219493433833122, 0.32706573605537415, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.310468852519989, 0.1188773512840271, 0.05778200551867485, 0.09368079900741577, 0.06338045001029968, 0.04212843254208565, 0.03693113103508949, 0.27675098180770874, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3214319944381714, 0.09294536709785461, 0.06742903590202332, 0.14854663610458374, 0.03368179500102997, 0.02713111601769924, 0.028831569477915764, 0.03549030050635338, 0.24451227486133575, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2825346887111664, 0.06728609651327133, 0.04610508307814598, 0.1224469318985939, 0.11661996692419052, 0.027326805517077446, 0.03303062915802002, 0.031494561582803726, 0.04793715104460716, 0.22521811723709106, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.22237195074558258, 0.07869469374418259, 0.06115278601646423, 0.08753293752670288, 0.04513182118535042, 0.03835989534854889, 0.03236133232712746, 0.036704059690237045, 0.0485537014901638, 0.06849370896816254, 0.280643105506897, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.23897147178649902, 0.07143733650445938, 0.04690999165177345, 0.09383968263864517, 0.058969832956790924, 0.037533316761255264, 0.033642083406448364, 0.04288466274738312, 0.08469031006097794, 0.04596831649541855, 0.04853065684437752, 0.19662226736545563, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1549462527036667, 0.06473598629236221, 0.04342089220881462, 0.07044702023267746, 0.048833876848220825, 0.1301896721124649, 0.03134886920452118, 0.03392164781689644, 0.06551963835954666, 0.027450481429696083, 0.04273710772395134, 0.06535234302282333, 0.22109626233577728, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.16590821743011475, 0.0476837195456028, 0.03471733257174492, 0.06866194307804108, 0.04168432578444481, 0.037712760269641876, 0.05232592672109604, 0.03686702251434326, 0.04889447242021561, 0.04286665469408035, 0.0527162104845047, 0.05623235926032066, 0.0403827428817749, 0.27334627509117126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.18517738580703735, 0.0678047314286232, 0.046660132706165314, 0.0621989443898201, 0.04051893576979637, 0.04304435849189758, 0.04213724657893181, 0.040838323533535004, 0.07514885812997818, 0.04641132429242134, 0.04953101649880409, 0.07730673998594284, 0.04277976229786873, 0.04658951982855797, 0.1338527649641037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.22398501634597778, 0.06980612874031067, 0.04371744021773338, 0.06406375020742416, 0.04192064702510834, 0.030514441430568695, 0.019981151446700096, 0.02868005633354187, 0.06162312999367714, 0.0335824191570282, 0.025781843811273575, 0.059996336698532104, 0.01562671549618244, 0.019640905782580376, 0.0550788976252079, 0.20600111782550812, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11970885843038559, 0.21628879010677338, 0.07848384976387024, 0.044578783214092255, 0.017328975722193718, 0.02730775810778141, 0.02072077803313732, 0.02221057005226612, 0.030436471104621887, 0.019141271710395813, 0.022519612684845924, 0.03648039698600769, 0.019534878432750702, 0.019076859578490257, 0.04500192031264305, 0.03184324875473976, 0.2293369472026825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1533554345369339, 0.040876444429159164, 0.03626977652311325, 0.04479736462235451, 0.034489862620830536, 0.019105179235339165, 0.023319752886891365, 0.030225606635212898, 0.05271529406309128, 0.026051126420497894, 0.02971794269979, 0.049838606268167496, 0.016767630353569984, 0.020260194316506386, 0.06908540427684784, 0.06739655137062073, 0.04472968354821205, 0.24099810421466827, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.13306719064712524, 0.05208565294742584, 0.03305720537900925, 0.03615260869264603, 0.03657248616218567, 0.03205621987581253, 0.022378673776984215, 0.07777445018291473, 0.03157679736614227, 0.018184537068009377, 0.019129537045955658, 0.05138583481311798, 0.025764260441064835, 0.0323905348777771, 0.05528810992836952, 0.0478813573718071, 0.05670929327607155, 0.04203714430332184, 0.19650804996490479, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11337292939424515, 0.04671236872673035, 0.03086916171014309, 0.06276978552341461, 0.03026672825217247, 0.027945369482040405, 0.023643964901566505, 0.03350434824824333, 0.05961280316114426, 0.03217153251171112, 0.03379756957292557, 0.06584880501031876, 0.026648027822375298, 0.03432459756731987, 0.0707424208521843, 0.047239601612091064, 0.06162748485803604, 0.06538116186857224, 0.030179738998413086, 0.10334169864654541, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09954793006181717, 0.059185732156038284, 0.028447875753045082, 0.02775123529136181, 0.02841828018426895, 0.039067309349775314, 0.0331152118742466, 0.04082687199115753, 0.02468639239668846, 0.013695069588720798, 0.01856249012053013, 0.03470142185688019, 0.02820705808699131, 0.023937935009598732, 0.05023195222020149, 0.026479221880435944, 0.05659688264131546, 0.04652110114693642, 0.06622873991727829, 0.04653767868876457, 0.20725354552268982, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10111834108829498, 0.037112317979335785, 0.026943888515233994, 0.04567815735936165, 0.024847732856869698, 0.024771839380264282, 0.028455538675189018, 0.034052859991788864, 0.05143488943576813, 0.026425477117300034, 0.02887629345059395, 0.05658871307969093, 0.02574199065566063, 0.03072414919734001, 0.06925953179597855, 0.040277935564517975, 0.054723672568798065, 0.05278732255101204, 0.02906104177236557, 0.08347661793231964, 0.04181433096528053, 0.0858273059129715, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07637831568717957, 0.018550440669059753, 0.009721602313220501, 0.027274003252387047, 0.038629982620477676, 0.025096390396356583, 0.043900277465581894, 0.030356543138623238, 0.02235221490263939, 0.04493856057524681, 0.031606048345565796, 0.03570156171917915, 0.020639795809984207, 0.042412444949150085, 0.04542623832821846, 0.030819861218333244, 0.020836887881159782, 0.03111150674521923, 0.03050607442855835, 0.048383381217718124, 0.03154255077242851, 0.04844684153795242, 0.24536855518817902, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1001521497964859, 0.025539202615618706, 0.017489762976765633, 0.042140573263168335, 0.03696268051862717, 0.03090972639620304, 0.018365351483225822, 0.040095649659633636, 0.02624949812889099, 0.03582436963915825, 0.0174917820841074, 0.035023581236600876, 0.016384921967983246, 0.02446676790714264, 0.04158612713217735, 0.019031887874007225, 0.026739802211523056, 0.023207994177937508, 0.02248014137148857, 0.04301014170050621, 0.03337562084197998, 0.04880525544285774, 0.02205260843038559, 0.25261440873146057, 0.0, 0.0, 0.0, 0.0], [0.10352129489183426, 0.030716916546225548, 0.024778015911579132, 0.04340185225009918, 0.0196408499032259, 0.020496932789683342, 0.02903309091925621, 0.02397630736231804, 0.036181334406137466, 0.029651742428541183, 0.02396252006292343, 0.05276242271065712, 0.019426411017775536, 0.025066282600164413, 0.05688608065247536, 0.032078154385089874, 0.03348727524280548, 0.032164525240659714, 0.018911076709628105, 0.07095654308795929, 0.02174951136112213, 0.0694187805056572, 0.026956375688314438, 0.03763768821954727, 0.11713789403438568, 0.0, 0.0, 0.0], [0.09517154842615128, 0.029134400188922882, 0.017753254622220993, 0.02978367730975151, 0.03044879250228405, 0.027293119579553604, 0.02779742144048214, 0.02169813960790634, 0.02324158512055874, 0.028059452772140503, 0.026290688663721085, 0.039718836545944214, 0.01879325695335865, 0.017560653388500214, 0.03967246413230896, 0.0413651205599308, 0.029916588217020035, 0.023970823734998703, 0.019553061574697495, 0.04660544916987419, 0.02968308888375759, 0.04537811130285263, 0.035625845193862915, 0.04123678430914879, 0.03751836717128754, 0.17672936618328094, 0.0, 0.0], [0.06384532153606415, 0.018449801951646805, 0.019877558574080467, 0.02718479558825493, 0.029620012268424034, 0.037329789251089096, 0.015898490324616432, 0.01845209300518036, 0.03462096303701401, 0.023472778499126434, 0.02659214287996292, 0.02855658158659935, 0.06745805591344833, 0.031369954347610474, 0.037086594849824905, 0.029814867302775383, 0.02380075491964817, 0.023642877116799355, 0.02548181638121605, 0.03325587138533592, 0.0275802593678236, 0.03686501085758209, 0.025933748111128807, 0.030576014891266823, 0.02653016522526741, 0.0236370712518692, 0.21306651830673218, 0.0], [0.06543552875518799, 0.01993795484304428, 0.012469192035496235, 0.01902545616030693, 0.015323041006922722, 0.014538892544806004, 0.017373591661453247, 0.013668007217347622, 0.026753226295113564, 0.01558497454971075, 0.016538644209504128, 0.03532310947775841, 0.01647944562137127, 0.015006864443421364, 0.05479054898023605, 0.03571408987045288, 0.0335531048476696, 0.0503336526453495, 0.03665418177843094, 0.050087958574295044, 0.03217364475131035, 0.051349349319934845, 0.02640591748058796, 0.02391018345952034, 0.04014714062213898, 0.03330785036087036, 0.026063967496156693, 0.20205053687095642]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8253123164176941, 0.17468766868114471, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7580504417419434, 0.1302967220544815, 0.1116529107093811, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7171333432197571, 0.07894084602594376, 0.07489750534296036, 0.1290283054113388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5856688022613525, 0.12062828242778778, 0.06587229669094086, 0.11469876021146774, 0.11313190311193466, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5836156010627747, 0.059149209409952164, 0.06341482698917389, 0.13287146389484406, 0.0682215541601181, 0.09272732585668564, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.43137127161026, 0.10361166298389435, 0.05670538544654846, 0.06270000338554382, 0.10235929489135742, 0.07021576166152954, 0.1730366051197052, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.36127617955207825, 0.10565770417451859, 0.056940462440252304, 0.0839114561676979, 0.07698319107294083, 0.1149652972817421, 0.13141928613185883, 0.0688464418053627, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.40861448645591736, 0.04145338758826256, 0.043926071375608444, 0.0588640421628952, 0.06792576611042023, 0.07547777891159058, 0.10901796817779541, 0.06660313159227371, 0.1281173676252365, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3805311620235443, 0.05003545805811882, 0.0473254956305027, 0.05294194445014, 0.06341471523046494, 0.06584890186786652, 0.08238597214221954, 0.0782962515950203, 0.09226129204034805, 0.08695877343416214, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3643495738506317, 0.09677403420209885, 0.04489242658019066, 0.0555158406496048, 0.06110784038901329, 0.04649265855550766, 0.05086726322770119, 0.04658419266343117, 0.08907953649759293, 0.07836185395717621, 0.06597478687763214, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.289888858795166, 0.035202983766794205, 0.04012434557080269, 0.05380578711628914, 0.052349332720041275, 0.07252711057662964, 0.10180802643299103, 0.05165955796837807, 0.08781175315380096, 0.08965005725622177, 0.09220664203166962, 0.032965537160634995, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2649606466293335, 0.057252272963523865, 0.03690424934029579, 0.03925821930170059, 0.08194616436958313, 0.044573232531547546, 0.07373521476984024, 0.05154817923903465, 0.07831669598817825, 0.06432931125164032, 0.07034994661808014, 0.04735894501209259, 0.08946692943572998, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.31049948930740356, 0.03833388909697533, 0.04195890948176384, 0.03286338597536087, 0.04475685954093933, 0.03379271551966667, 0.07416792958974838, 0.04845641925930977, 0.0673341229557991, 0.06234121322631836, 0.05485284700989723, 0.054233573377132416, 0.0791928693652153, 0.05721588060259819, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2678288519382477, 0.030415069311857224, 0.035894472151994705, 0.041633687913417816, 0.04905315116047859, 0.04782307893037796, 0.06612762808799744, 0.04764653742313385, 0.06244067847728729, 0.06227368861436844, 0.07205259799957275, 0.031117653474211693, 0.06096348166465759, 0.04326754808425903, 0.0814618393778801, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.18630345165729523, 0.030082643032073975, 0.017054185271263123, 0.049566611647605896, 0.02207273244857788, 0.03757476061582565, 0.06445390731096268, 0.029004612937569618, 0.08657916635274887, 0.06163901463150978, 0.06528154015541077, 0.0376676581799984, 0.04142848774790764, 0.09211321175098419, 0.1273072212934494, 0.05187084153294563, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.21882383525371552, 0.027198854833841324, 0.02620723471045494, 0.04345296323299408, 0.04118506982922554, 0.046696003526449203, 0.0676337406039238, 0.04601183906197548, 0.05872669816017151, 0.06603094190359116, 0.07533489167690277, 0.02509063296020031, 0.05746449530124664, 0.04179326444864273, 0.09878265857696533, 0.04660186544060707, 0.012965074740350246, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2858024537563324, 0.0275025125592947, 0.04154496639966965, 0.03513750806450844, 0.038452304899692535, 0.0389404296875, 0.044020526111125946, 0.04364105686545372, 0.06676148623228073, 0.04543554410338402, 0.05624751001596451, 0.030609767884016037, 0.04864683747291565, 0.02473931759595871, 0.05876825004816055, 0.0480448380112648, 0.009534155949950218, 0.0561705157160759, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2369573563337326, 0.04926378279924393, 0.030704211443662643, 0.04281195253133774, 0.03724413737654686, 0.024210378527641296, 0.048617444932460785, 0.025096114724874496, 0.07512000948190689, 0.05664016306400299, 0.052388861775398254, 0.02847067266702652, 0.03506385535001755, 0.03270477429032326, 0.053802162408828735, 0.07488387078046799, 0.02201071009039879, 0.0502079501748085, 0.023801587522029877, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.23444850742816925, 0.023378070443868637, 0.032466936856508255, 0.03983627259731293, 0.03619704023003578, 0.03517034277319908, 0.05582454055547714, 0.034004997462034225, 0.05468001589179039, 0.043294284492731094, 0.047863055020570755, 0.025000113993883133, 0.04820922389626503, 0.03996620327234268, 0.05373525992035866, 0.04569926857948303, 0.010620529763400555, 0.04913958162069321, 0.03899814933538437, 0.05146753415465355, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.18807010352611542, 0.04424242302775383, 0.030884208157658577, 0.03355782851576805, 0.03355103358626366, 0.018242796882987022, 0.03904589265584946, 0.022020962089300156, 0.08156836777925491, 0.04666345939040184, 0.025101782754063606, 0.05329466238617897, 0.03163645789027214, 0.023175248876214027, 0.08598220348358154, 0.05560668557882309, 0.027005810290575027, 0.02827470563352108, 0.019991176202893257, 0.08955229073762894, 0.022531883791089058, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.19371438026428223, 0.021206194534897804, 0.028638001531362534, 0.03097751922905445, 0.032570045441389084, 0.03582385927438736, 0.0515863336622715, 0.031488578766584396, 0.0566822811961174, 0.04240713641047478, 0.04148433730006218, 0.02475420944392681, 0.038088683038949966, 0.028869519010186195, 0.055579185485839844, 0.04744622856378555, 0.008517525158822536, 0.04001771658658981, 0.041494473814964294, 0.04969276115298271, 0.04393927752971649, 0.05502175912261009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.16350007057189941, 0.036435216665267944, 0.03603365644812584, 0.034274183213710785, 0.03624330833554268, 0.018478229641914368, 0.03277553990483284, 0.027863729745149612, 0.06459066271781921, 0.036880481988191605, 0.017793195322155952, 0.07002860307693481, 0.02828024886548519, 0.016960419714450836, 0.07328850030899048, 0.0378311350941658, 0.017849724739789963, 0.029655277729034424, 0.029399169608950615, 0.06059807538986206, 0.022950343787670135, 0.0569043830037117, 0.051385778933763504, 0.0, 0.0, 0.0, 0.0, 0.0], [0.18945816159248352, 0.044774096459150314, 0.028758276253938675, 0.031868379563093185, 0.043209705501794815, 0.023037921637296677, 0.04231632128357887, 0.0380728542804718, 0.04810849577188492, 0.05834081023931503, 0.03566887974739075, 0.033507440239191055, 0.03315272182226181, 0.02190808206796646, 0.02802453748881817, 0.07776319235563278, 0.014796275645494461, 0.018706990405917168, 0.033506426960229874, 0.03299469128251076, 0.024737266823649406, 0.02488420158624649, 0.04948827624320984, 0.02291598729789257, 0.0, 0.0, 0.0, 0.0], [0.16335764527320862, 0.022478962317109108, 0.022471385076642036, 0.04394601657986641, 0.02851354144513607, 0.0338900089263916, 0.047426797449588776, 0.03188341483473778, 0.03763141483068466, 0.04594678431749344, 0.04740944504737854, 0.015576119534671307, 0.03231525048613548, 0.03622175008058548, 0.037035055458545685, 0.04155708849430084, 0.008913434110581875, 0.05026906728744507, 0.036150239408016205, 0.04032828286290169, 0.039605915546417236, 0.03600206598639488, 0.04484760761260986, 0.030646583065390587, 0.025576110929250717, 0.0, 0.0, 0.0], [0.22925211489200592, 0.01844448409974575, 0.023297257721424103, 0.05811448395252228, 0.02223830483853817, 0.024948276579380035, 0.026244504377245903, 0.028433604165911674, 0.045685358345508575, 0.03773258998990059, 0.04499594494700432, 0.009943214245140553, 0.02126915007829666, 0.02262754552066326, 0.0388946495950222, 0.03227026388049126, 0.006539344321936369, 0.05161161720752716, 0.02825631946325302, 0.03941921517252922, 0.03189939260482788, 0.03731773793697357, 0.035477425903081894, 0.03238316625356674, 0.021381132304668427, 0.03132287412881851, 0.0, 0.0], [0.1783008575439453, 0.026671092957258224, 0.021505434066057205, 0.020543523132801056, 0.02472810633480549, 0.02398056350648403, 0.054837603121995926, 0.030408309772610664, 0.0334751270711422, 0.03892194852232933, 0.05347047001123428, 0.026079580187797546, 0.04729043319821358, 0.04529455676674843, 0.0445893332362175, 0.052273452281951904, 0.013444194570183754, 0.014936435036361217, 0.017785102128982544, 0.03092864714562893, 0.021100929006934166, 0.021761801093816757, 0.05697048455476761, 0.01673748716711998, 0.02052251622080803, 0.031161613762378693, 0.03228031098842621, 0.0], [0.1263667792081833, 0.016784176230430603, 0.02339768223464489, 0.03007865883409977, 0.03027958609163761, 0.04747075214982033, 0.06621608138084412, 0.03349040821194649, 0.03225644677877426, 0.04531002417206764, 0.04156625643372536, 0.009043537080287933, 0.04517446830868721, 0.020954877138137817, 0.0437738262116909, 0.02729899436235428, 0.007021595258265734, 0.024649208411574364, 0.03423381224274635, 0.02767028659582138, 0.03808377683162689, 0.02276751399040222, 0.04115179181098938, 0.035905104130506516, 0.017846399918198586, 0.026665782555937767, 0.046902015805244446, 0.037640176713466644]]]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x339d45110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import circuitsvis as cv\n",
    "from IPython.display import display\n",
    "\n",
    "reference_text = \"I am a powerful language model trained on massive amounts of text data. Soon I will learn to understand and generate human-like content!\"\n",
    "tokens = reference_gpt2.to_tokens(reference_text).to(device)\n",
    "logits, cache = reference_gpt2.run_with_cache(tokens)\n",
    "\n",
    "# Attention pattern of layer 0 token 0\n",
    "display(\n",
    "    cv.attention.attention_patterns(\n",
    "        tokens=reference_gpt2.to_str_tokens(reference_text), attention=cache[\"pattern\", 0][0]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Attention Architecture](../nanoGPT/transformer-attention-architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    IGNORE: Float[Tensor, \"\"]\n",
    "\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_Q = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        self.W_K = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        self.W_V = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        self.W_O = nn.Parameter(t.empty((cfg.n_heads, cfg.d_head, cfg.d_model)))\n",
    "        self.b_Q = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_K = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_V = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_O = nn.Parameter(t.zeros((cfg.d_model)))\n",
    "        nn.init.normal_(self.W_Q, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_K, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_V, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_O, std=self.cfg.init_range)\n",
    "        self.register_buffer(\"IGNORE\", t.tensor(float(\"-inf\"), dtype=t.float32, device=device))\n",
    "\n",
    "    def forward(self, normalized_resid_pre: Float[Tensor, \"batch posn d_model\"]) -> Float[Tensor, \"batch posn d_model\"]:\n",
    "        # Calculate query, key and value vectors\n",
    "        q = (\n",
    "            einops.einsum(\n",
    "                normalized_resid_pre, self.W_Q, \"batch posn d_model, nheads d_model d_head -> batch posn nheads d_head\"\n",
    "            )\n",
    "            + self.b_Q\n",
    "        )\n",
    "        k = (\n",
    "            einops.einsum(\n",
    "                normalized_resid_pre, self.W_K, \"batch posn d_model, nheads d_model d_head -> batch posn nheads d_head\"\n",
    "            )\n",
    "            + self.b_K\n",
    "        )\n",
    "        v = (\n",
    "            einops.einsum(\n",
    "                normalized_resid_pre, self.W_V, \"batch posn d_model, nheads d_model d_head -> batch posn nheads d_head\"\n",
    "            )\n",
    "            + self.b_V\n",
    "        )\n",
    "\n",
    "        # Calculate attention scores, then scale and mask, and apply softmax to get probabilities\n",
    "        attn_scores = einops.einsum(\n",
    "            q, k, \"batch posn_Q nheads d_head, batch posn_K nheads d_head -> batch nheads posn_Q posn_K\"\n",
    "        )\n",
    "        attn_scores_masked = self.apply_causal_mask(attn_scores / self.cfg.d_head**0.5)\n",
    "        attn_pattern = attn_scores_masked.softmax(-1)\n",
    "\n",
    "        # Take weighted sum of value vectors, according to attention probabilities\n",
    "        z = einops.einsum(\n",
    "            v, attn_pattern, \"batch posn_K nheads d_head, batch nheads posn_Q posn_K -> batch posn_Q nheads d_head\"\n",
    "        )\n",
    "\n",
    "        # Calculate output (by applying matrix W_O and summing over heads, then adding bias b_O)\n",
    "        attn_out = (\n",
    "            einops.einsum(z, self.W_O, \"batch posn_Q nheads d_head, nheads d_head d_model -> batch posn_Q d_model\")\n",
    "            + self.b_O\n",
    "        )\n",
    "\n",
    "        return attn_out\n",
    "\n",
    "    def apply_causal_mask(\n",
    "        self, attn_scores: Float[Tensor, \"batch n_heads query_pos key_pos\"]\n",
    "    ) -> Float[Tensor, \"batch n_heads query_pos key_pos\"]:\n",
    "        \"\"\"\n",
    "        Applies a causal mask to attention scores, and returns masked scores.\n",
    "        \"\"\"\n",
    "        # Define a mask that is True for all positions we want to set probabilities to zero for\n",
    "        all_ones = t.ones(attn_scores.size(-2), attn_scores.size(-1), device=attn_scores.device)\n",
    "        mask = t.triu(all_ones, diagonal=1).bool()\n",
    "        # Apply the mask to attention scores, then return the masked scores\n",
    "        attn_scores.masked_fill_(mask, self.IGNORE)\n",
    "        return attn_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP (Multi-Layer Perceptron)\n",
    "\n",
    "The MLP component implements a standard feedforward neural network architecture with a single hidden layer and GELU activation function. Following transformer conventions, the hidden dimension is typically 4x the model dimension (d_mlp = 4 * d_model).\n",
    "\n",
    "A fundamental property of MLPs is that they process each position in the residual stream independently and identically - unlike attention layers, they don't transfer information between positions. This makes them ideal for processing information that attention has already gathered to specific positions.\n",
    "\n",
    "MLPs can be understood through multiple lenses:\n",
    "1. As key-value memory systems: Input weights act as \"keys\" detecting specific features, while output weights serve as \"values\" that get activated\n",
    "2. As knowledge storage: MLPs store learned patterns and information, processing inputs to write derived information into the residual stream\n",
    "3. As memory managers: Certain neurons may help manage the residual stream's capacity by selectively erasing specific vector components\n",
    "\n",
    "The exact implementation uses GELU activation which empirically performs well, though the specific activation function isn't conceptually critical. The architecture follows historical precedent set by early transformer models.\n",
    "\n",
    "![MLP Architecture](../nanoGPT/mlp-architecture.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_in = nn.Parameter(t.empty((cfg.d_model, cfg.d_mlp)))\n",
    "        self.W_out = nn.Parameter(t.empty((cfg.d_mlp, cfg.d_model)))\n",
    "        self.b_in = nn.Parameter(t.zeros((cfg.d_mlp)))\n",
    "        self.b_out = nn.Parameter(t.zeros((cfg.d_model)))\n",
    "        nn.init.normal_(self.W_in, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_out, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(self, normalized_resid_mid: Float[Tensor, \"batch posn d_model\"]) -> Float[Tensor, \"batch posn d_model\"]:\n",
    "        pre = (\n",
    "            einops.einsum(\n",
    "                normalized_resid_mid, self.W_in, \"batch position d_model, d_model d_mlp -> batch position d_mlp\"\n",
    "            )\n",
    "            + self.b_in\n",
    "        )\n",
    "        post = gelu_new(pre)\n",
    "        mlp_out = (\n",
    "            einops.einsum(post, self.W_out, \"batch position d_mlp, d_mlp d_model -> batch position d_model\")\n",
    "            + self.b_out\n",
    "        )\n",
    "        return mlp_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Block\n",
    "\n",
    "![Transformer Block](../nanoGPT/transformer-architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.ln1 = LayerNorm(cfg)\n",
    "        self.attn = Attention(cfg)\n",
    "        self.ln2 = LayerNorm(cfg)\n",
    "        self.mlp = MLP(cfg)\n",
    "\n",
    "    def forward(self, resid_pre: Float[Tensor, \"batch position d_model\"]) -> Float[Tensor, \"batch position d_model\"]:\n",
    "        resid_mid = self.attn(self.ln1(resid_pre)) + resid_pre\n",
    "        resid_post = self.mlp(self.ln2(resid_mid)) + resid_mid\n",
    "        return resid_post\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unembedding\n",
    "\n",
    "The unembedding layer is a simple linear layer that maps the model's internal representation back to the original token space. It's used to convert the output of the transformer into a sequence of tokens, which can then be used for further processing or decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unembed(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_U = nn.Parameter(t.empty((cfg.d_model, cfg.d_vocab)))\n",
    "        nn.init.normal_(self.W_U, std=self.cfg.init_range)\n",
    "        self.b_U = nn.Parameter(t.zeros((cfg.d_vocab), requires_grad=False))\n",
    "\n",
    "    def forward(\n",
    "        self, normalized_resid_final: Float[Tensor, \"batch position d_model\"]\n",
    "    ) -> Float[Tensor, \"batch position d_vocab\"]:\n",
    "        return (\n",
    "            einops.einsum(\n",
    "                normalized_resid_final,\n",
    "                self.W_U,\n",
    "                \"batch posn d_model, d_model d_vocab -> batch posn d_vocab\",\n",
    "            )\n",
    "            + self.b_U\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full GPT Model\n",
    "\n",
    "![Full GPT Model](../nanoGPT/high_level_architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, cfg: Config):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.embed = Embed(cfg)\n",
    "        self.pos_embed = PosEmbed(cfg)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(cfg.n_layers)])\n",
    "        self.ln_final = LayerNorm(cfg)\n",
    "        self.unembed = Unembed(cfg)\n",
    "\n",
    "    def forward(self, tokens: Int[Tensor, \"batch position\"]) -> Float[Tensor, \"batch position d_vocab\"]:\n",
    "        residual = self.embed(tokens) + self.pos_embed(tokens)\n",
    "        for block in self.blocks:\n",
    "            residual = block(residual)\n",
    "        logits = self.unembed(self.ln_final(residual))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_gpt2 = GPT(Config(debug=False)).to(device)\n",
    "demo_gpt2.load_state_dict(reference_gpt2.state_dict(), strict=False)\n",
    "\n",
    "demo_logits = demo_gpt2(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg cross entropy loss: 4.2215\n",
      "Avg cross entropy loss for uniform distribution: 10.824905\n",
      "Avg probability assigned to correct token: 0.128396\n"
     ]
    }
   ],
   "source": [
    "def get_log_probs(\n",
    "    logits: Float[Tensor, \"batch posn d_vocab\"], tokens: Int[Tensor, \"batch posn\"]\n",
    ") -> Float[Tensor, \"batch posn-1\"]:\n",
    "    log_probs = logits.log_softmax(dim=-1)\n",
    "    # Get logprobs the first seq_len-1 predictions (so we can compare them with the actual next tokens)\n",
    "    log_probs_for_tokens = log_probs[:, :-1].gather(dim=-1, index=tokens[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    return log_probs_for_tokens\n",
    "\n",
    "\n",
    "pred_log_probs = get_log_probs(demo_logits, tokens)\n",
    "print(f\"Avg cross entropy loss: {-pred_log_probs.mean():.4f}\")\n",
    "print(f\"Avg cross entropy loss for uniform distribution: {math.log(demo_gpt2.cfg.d_vocab):4f}\")\n",
    "print(f\"Avg probability assigned to correct token: {pred_log_probs.exp().mean():4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab238da2f4f44734badd00567cfebd74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Perspective Vortex derives its picture of the whole Universe on the principle of the total perspective. The total perspective is the view of the whole Universe from the point of view of the observer. The total perspective is the view of the whole Universe from the point of view of the observer. The total perspective is the view of the whole Universe from the point of view of the observer. The total perspective is the view of the whole Universe from the point of view of the observer. The total perspective is the view of the whole Universe from the point of view of the observer. The\n"
     ]
    }
   ],
   "source": [
    "test_string = \"\"\"The Total Perspective Vortex derives its picture of the whole Universe on the principle of\"\"\"\n",
    "for i in tqdm(range(100)):\n",
    "    test_tokens = reference_gpt2.to_tokens(test_string).to(device)\n",
    "    demo_logits = demo_gpt2(test_tokens)\n",
    "    test_string += reference_gpt2.tokenizer.decode(demo_logits[-1, -1].argmax())\n",
    "\n",
    "print(test_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model appears to be working reasonably well:\n",
    "1. The cross entropy loss is significantly better than random (uniform distribution)\n",
    "2. The model assigns meaningful probabilities to correct tokens\n",
    "3. The text generation example shows coherent continuation, though it may not be perfectly fluent or logically consistent\n",
    "4. The model successfully loaded the pretrained weights and produces sensible outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "## Configurations\n",
    "\n",
    "The next code block defines model configuration and training arguments:\n",
    "- Creates a smaller GPT model with 256 dim embeddings, 4 attention heads, 2 layers\n",
    "- Sets up training hyperparameters like batch size, learning rate, etc.\n",
    "- Configures optional Weights & Biases logging\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cfg = Config(\n",
    "    debug=False,\n",
    "    d_model=256,\n",
    "    n_heads=4,\n",
    "    d_head=64,\n",
    "    d_mlp=1024,\n",
    "    n_layers=2,\n",
    "    n_ctx=256,\n",
    "    d_vocab=50257,\n",
    ")\n",
    "model = GPT(model_cfg)\n",
    "\n",
    "@dataclass\n",
    "class TransformerTrainingArgs:\n",
    "    batch_size = 16\n",
    "    epochs = 20\n",
    "    max_steps_per_epoch = 200\n",
    "    lr = 1e-3\n",
    "    weight_decay = 1e-2\n",
    "    wandb_project: str | None = \"cleangpt2\"\n",
    "    wandb_name: str | None = None\n",
    "\n",
    "\n",
    "args = TransformerTrainingArgs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "The code below loads a small subset of The Pile dataset (10k examples) from the Hugging Face Hub\n",
    "This provides training data for fine-tuning our GPT-2 model\n",
    "The dataset contains text samples that we'll use to train the model on next-token prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 10000\n",
      "})\n",
      "It is done, and submitted. You can play “Survival of the Tastiest” on Android, and on the web. Playi\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"NeelNanda/pile-10k\", split=\"train\").remove_columns(\"meta\")\n",
    "print(dataset)\n",
    "print(dataset[0][\"text\"][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Processing\n",
    "1. Tokenizes the dataset using the GPT-2 tokenizer\n",
    "2. Concatenates sequences to the model's context length (n_ctx)\n",
    "3. Splits data into train and test sets\n",
    "4. Creates DataLoader objects for efficient batched training\n",
    "   - Uses multiple workers for parallel data loading\n",
    "   - Enables memory pinning for faster GPU transfer\n",
    "   - Shuffles training data while keeping test data ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "295528d5381148a797bb753982daa5d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (80023 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (101051 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (155995 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (229134 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = tokenize_and_concatenate(\n",
    "    dataset,\n",
    "    reference_gpt2.tokenizer,\n",
    "    streaming=False,\n",
    "    max_length=model.cfg.n_ctx,\n",
    "    column_name=\"text\",\n",
    "    add_bos_token=True,\n",
    "    num_proc=4,\n",
    ")\n",
    "\n",
    "dataset_dict = tokenized_dataset.train_test_split(test_size=1000)\n",
    "train_loader = DataLoader(\n",
    "    dataset_dict[\"train\"], batch_size=args.batch_size, shuffle=True, num_workers=4, pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset_dict[\"test\"], batch_size=args.batch_size, shuffle=False, num_workers=4, pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "\n",
    "The code below defines a TransformerTrainer class for training the GPT model:\n",
    "- Initializes optimizer and data loaders for training and testing\n",
    "- Defines training_step method to:\n",
    "  - Compute model predictions and loss\n",
    "  - Perform backpropagation and optimization step\n",
    "  - Log training loss to Weights & Biases\n",
    "- Defines evaluate method to compute model accuracy on test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerTrainer:\n",
    "    def __init__(self, args: TransformerTrainingArgs, model: GPT):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.args = args\n",
    "\n",
    "        self.optimizer = t.optim.AdamW(self.model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "        self.step = 0\n",
    "\n",
    "        self.train_loader = DataLoader(\n",
    "            dataset_dict[\"train\"], batch_size=args.batch_size, shuffle=True, num_workers=4, pin_memory=True\n",
    "        )\n",
    "        self.test_loader = DataLoader(\n",
    "            dataset_dict[\"test\"], batch_size=args.batch_size, shuffle=False, num_workers=4, pin_memory=True\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch: dict[str, Int[Tensor, \"batch seq\"]]) -> Float[Tensor, \"\"]:\n",
    "        \"\"\"\n",
    "        Calculates the loss on the tokens in the batch, performs a gradient update step, and logs the loss.\n",
    "\n",
    "        Remember that `batch` is a dictionary with the single key 'tokens'.\n",
    "        \"\"\"\n",
    "        tokens = batch[\"tokens\"].to(device)\n",
    "        logits = self.model(tokens)\n",
    "        loss = -get_log_probs(logits, tokens).mean()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "        self.step += 1\n",
    "        wandb.log({\"train_loss\": loss}, step=self.step)\n",
    "        return loss\n",
    "\n",
    "    @t.inference_mode()\n",
    "    def evaluate(self) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate the model on the test set and return the accuracy.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        total_correct, total_samples = 0, 0\n",
    "\n",
    "        for batch in tqdm(self.test_loader, desc=\"Evaluating\"):\n",
    "            tokens = batch[\"tokens\"].to(device)\n",
    "            logits: Tensor = self.model(tokens)[:, :-1]\n",
    "            predicted_tokens = logits.argmax(dim=-1)\n",
    "            total_correct += (predicted_tokens == tokens[:, 1:]).sum().item()\n",
    "            total_samples += tokens.size(0) * (tokens.size(1) - 1)\n",
    "\n",
    "        accuracy = total_correct / total_samples\n",
    "        wandb.log({\"accuracy\": accuracy}, step=self.step)\n",
    "        return accuracy\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Trains the model, for `self.args.epochs` epochs. Also handles wandb initialisation, and early stopping\n",
    "        for each epoch at `self.args.max_steps_per_epoch` steps.\n",
    "        \"\"\"\n",
    "        wandb.init(project=self.args.wandb_project, name=self.args.wandb_name, config=self.args)\n",
    "        accuracy = np.nan\n",
    "\n",
    "        progress_bar = tqdm(total=self.args.max_steps_per_epoch * self.args.epochs)\n",
    "\n",
    "        for epoch in range(self.args.epochs):\n",
    "            for i, batch in enumerate(self.train_loader):\n",
    "                loss = self.training_step(batch)\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description(f\"Epoch {epoch+1}, loss: {loss:.3f}, accuracy: {accuracy:.3f}\")\n",
    "                if i >= self.args.max_steps_per_epoch:\n",
    "                    break\n",
    "\n",
    "            accuracy = self.evaluate()\n",
    "\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmichaelyliu6\u001b[0m (\u001b[33mmichaelyliu6-none\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/michaelliu/workspace/gpt2-124M-clone/gpt2-124M-clone/cleanGPT/wandb/run-20250119_012833-wo2vtqqo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/michaelyliu6-none/cleangpt2/runs/wo2vtqqo' target=\"_blank\">balmy-totem-1</a></strong> to <a href='https://wandb.ai/michaelyliu6-none/cleangpt2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/michaelyliu6-none/cleangpt2' target=\"_blank\">https://wandb.ai/michaelyliu6-none/cleangpt2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/michaelyliu6-none/cleangpt2/runs/wo2vtqqo' target=\"_blank\">https://wandb.ai/michaelyliu6-none/cleangpt2/runs/wo2vtqqo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df9d39bf479c4c7eb14011f160d52b66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6732571134b94cb1b8e85c615b88c158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "117ac6b617a7441b8308a457167deca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "713243e87d0e4684876105a0b8132997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b5ec6aa9b37433688fae0361dca9943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9592e669cbe94bf7ac9c4e5ad97b57be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27a7d3493425464a88d88e02646aa550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1840f0b2c0524747896be16fcc65a7bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f99de692be6c4026b7970479f9275ae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ec87f69fbc74d8f85e9e218817a5373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba0ca5ad5a5f4dc79c94b4e2b7f733c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7534eeb47584617bf7e5846ed521b0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9215952e300e45f68fc30a17dd6831b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2fb2c879da54ddb9513348fdaeda72a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ace0fc6b9e1485f9dd71841c1ddd048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a29054323094509b81cc52b0a3964de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7902d4d82f11403187ec492ebba6947f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b415d103f6e94688ac985fb7b1f2c014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2079cd1551a2439d965147abfcf57a34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4201f6f1e2664815b811c60842bb2b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c4f55927de1443e9ac22f66c0408ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▃▃▄▄▅▅▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train_loss</td><td>█▅█▇▅▇▅▅▅▅▄▅▃▄▅▃▂▄▂▃▃▄▂▃▄▂▂▄▃▂▃▃▃▄▃▂▁▃▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.31804</td></tr><tr><td>train_loss</td><td>4.23696</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">balmy-totem-1</strong> at: <a href='https://wandb.ai/michaelyliu6-none/cleangpt2/runs/wo2vtqqo' target=\"_blank\">https://wandb.ai/michaelyliu6-none/cleangpt2/runs/wo2vtqqo</a><br> View project at: <a href='https://wandb.ai/michaelyliu6-none/cleangpt2' target=\"_blank\">https://wandb.ai/michaelyliu6-none/cleangpt2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250119_012833-wo2vtqqo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = GPT(model_cfg).to(device)\n",
    "args = TransformerTrainingArgs()\n",
    "trainer = TransformerTrainer(args, model)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
